{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9HPSAOgk4fz",
        "outputId": "6993da47-f253-4199-88ef-da959ade256a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.4.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.1.0)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.44)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.45)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (25.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.11.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "import ast\n",
        "import json\n",
        "import traceback\n",
        "from typing import TypedDict, List, Dict\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI Studio API Key: \")\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.graph import START, StateGraph, END\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "#                1. STATE FORMAT\n",
        "# -------------------------------------------------\n",
        "class ResearchState(TypedDict):\n",
        "    file_path: str\n",
        "    user_prompt: str\n",
        "    pdf_pages: list\n",
        "    split_sections: dict\n",
        "    plan: str\n",
        "    final_answer: str\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "#                     LLM\n",
        "# -------------------------------------------------\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "#                Utility\n",
        "# -------------------------------------------------\n",
        "def safe_join_pages(pages, max_chars=12000):\n",
        "    if not pages:\n",
        "        return \"\"\n",
        "    try:\n",
        "        joined = \"\\n\\n\".join([getattr(p, \"page_content\", str(p)) for p in pages])\n",
        "        return joined[:max_chars]\n",
        "    except Exception:\n",
        "        return str(pages)[:max_chars]\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "#                2.   NODES\n",
        "# -------------------------------------------------\n",
        "\n",
        "# ------------ PDF LOADER ------------\n",
        "def pdf_loader_node(state: ResearchState):\n",
        "    print(\"\\n[AGENT] pdf_loader running...\")\n",
        "    path = state.get(\"file_path\", \"\")\n",
        "    try:\n",
        "        loader = PyPDFLoader(path)\n",
        "        pages = loader.load()\n",
        "        print(\"[pdf_loader] Loaded PDF successfully.\")\n",
        "        return {\"pdf_pages\": pages}\n",
        "    except Exception as e:\n",
        "        print(\"[pdf_loader] ERROR loading PDF.\")\n",
        "        return {\"pdf_pages\": [f\"Error loading PDF: {str(e)}\"]}\n",
        "\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_string(s: str) -> str:\n",
        "    \"\"\"\n",
        "    Keep only safe characters:\n",
        "    A–Z a–z 0–9 space comma period dash\n",
        "    Remove anything else.\n",
        "    Collapse multiple spaces.\n",
        "    \"\"\"\n",
        "    allowed = re.sub(r\"[^A-Za-z0-9 ,.\\-]\", \" \", s)\n",
        "    allowed = re.sub(r\"\\s+\", \" \", allowed)\n",
        "    return allowed.strip()\n",
        "\n",
        "\n",
        "def splitter_agent_node(state: ResearchState):\n",
        "    print(\"\\n[AGENT] splitter running...\")\n",
        "    print(\"[LLM] Splitter agent calling LLM...\")\n",
        "\n",
        "    pages = state.get(\"pdf_pages\", [])\n",
        "    joined = safe_join_pages(pages, max_chars=15000)\n",
        "\n",
        "    system = \"\"\"\n",
        "You are a PDF Subsection Splitter Agent.\n",
        "\n",
        "Rules:\n",
        "- Split the paper into **10 to 15 subsections**.\n",
        "- Output MUST be a **valid Python dictionary literal**.\n",
        "- You are not allowed to miss any content from the paper.\n",
        "- Keys must be **simple strings**: letters, numbers, spaces only.\n",
        "- Values must be **simple text strings** with no special characters.\n",
        "- No markdown or bullet points.\n",
        "- No commentary.\n",
        "- Return ONLY the dictionary literal.\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        msg = [\n",
        "            SystemMessage(content=system),\n",
        "            HumanMessage(content=f\"PDF CONTENT:\\n{joined}\")\n",
        "        ]\n",
        "        response = llm.invoke(msg)\n",
        "        raw = response.content.strip()\n",
        "\n",
        "        # -------- TRY PYTHON DICT PARSE ----------\n",
        "        parsed = None\n",
        "        try:\n",
        "            parsed = ast.literal_eval(raw)\n",
        "        except Exception:\n",
        "            try:\n",
        "                parsed = json.loads(raw)\n",
        "            except Exception:\n",
        "                print(\"[splitter] FAILED to parse output.\")\n",
        "                return {\n",
        "                    \"split_sections\": {\n",
        "                        \"FullText\": joined,\n",
        "                        \"parse_error\": \"Could not parse splitter output\",\n",
        "                        \"raw_llm\": raw[:2000]\n",
        "                    }\n",
        "                }\n",
        "\n",
        "        if not isinstance(parsed, dict):\n",
        "            print(\"[splitter] Parsed output not a dict.\")\n",
        "            return {\n",
        "                \"split_sections\": {\n",
        "                    \"FullText\": joined,\n",
        "                    \"parse_error\": \"Output was not a dict\",\n",
        "                    \"raw_llm\": raw[:2000]\n",
        "                }\n",
        "            }\n",
        "\n",
        "        # -------- SANITIZE KEYS + VALUES ----------\n",
        "        cleaned = {}\n",
        "\n",
        "        for k, v in parsed.items():\n",
        "            k_clean = clean_string(str(k))\n",
        "            v_clean = clean_string(str(v))\n",
        "\n",
        "            if not k_clean:\n",
        "                k_clean = \"Section\"\n",
        "\n",
        "            while k_clean in cleaned:\n",
        "                k_clean += \"_2\"\n",
        "\n",
        "            cleaned[k_clean] = v_clean\n",
        "\n",
        "        print(\"[splitter] CLEANED keys:\", list(cleaned.keys()))\n",
        "        return {\"split_sections\": cleaned}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"[splitter] ERROR:\", str(e))\n",
        "        return {\"split_sections\": {\"error\": f\"Splitter agent error: {str(e)}\"}}\n",
        "\n",
        "\n",
        "\n",
        "# ------------ PLANNER ------------\n",
        "def planner_node(state: ResearchState):\n",
        "    print(\"\\n[AGENT] planner running...\")\n",
        "    print(\"[LLM] Planner agent calling LLM...\")\n",
        "\n",
        "    sections = state.get(\"split_sections\", {})\n",
        "    user_prompt = state.get(\"user_prompt\", \"\")\n",
        "\n",
        "    system = (\n",
        "        \"You are a Planning Agent.\\n\"\n",
        "        \"Use the provided section dictionary to make a step-by-step plan to answer the user query.\\n\"\n",
        "        \"Write the plan in clear numbered steps.\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        preview = {k: (v[:800] if isinstance(v, str) else str(v))\n",
        "                   for k, v in list(sections.items())[:10]}\n",
        "\n",
        "        msg = [\n",
        "            SystemMessage(content=system),\n",
        "            HumanMessage(content=f\"SECTIONS:\\n{preview}\\n\\nUSER PROMPT:\\n{user_prompt}\")\n",
        "        ]\n",
        "\n",
        "        response = llm.invoke(msg)\n",
        "        plan = response.content.strip()\n",
        "        print(\"[planner] LLM call completed.\")\n",
        "        return {\"plan\": plan or \"Planner returned empty plan.\"}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"[planner] ERROR:\", str(e))\n",
        "        return {\"plan\": f\"Planner error: {str(e)}\"}\n",
        "\n",
        "\n",
        "\n",
        "# ------------ FINAL ANSWER AGENT ------------\n",
        "def final_answer_node(state: ResearchState):\n",
        "    print(\"\\n[AGENT] final_answer running...\")\n",
        "    print(\"[LLM] Final answer agent calling LLM...\")\n",
        "\n",
        "    user_prompt = state.get(\"user_prompt\", \"\")\n",
        "    sections = state.get(\"split_sections\", {})\n",
        "    plan = state.get(\"plan\", \"\")\n",
        "\n",
        "    system = (\n",
        "        \"You are the Final Solution Agent.\\n\"\n",
        "        \"Your job is to produce the final answer STRICTLY by following the step-by-step plan.\\n\"\n",
        "        \"Mandatory rules:\\n\"\n",
        "        \"- Use ONLY the subsection dictionary from the Splitter Agent.\\n\"\n",
        "        \"- For every step, explicitly cite which subsections the content was extracted from.\\n\"\n",
        "        \"- If a step cannot be fulfilled from provided sections, state that the paper does not provide enough information.\\n\"\n",
        "        \"- Provide clear, concise, academically precise explanations.\\n\"\n",
        "        \"- The final answer MUST be written in numbered steps, one per step of the plan.\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        preview = {\n",
        "            k: (v[:1500] if isinstance(v, str) else str(v))\n",
        "            for k, v in list(sections.items())[:20]\n",
        "        }\n",
        "\n",
        "        msg = [\n",
        "            SystemMessage(content=system),\n",
        "            HumanMessage(\n",
        "                content=\n",
        "                f\"PLAN:\\n{plan}\\n\\n\"\n",
        "                f\"MAIN CONTENT (Subsections from Splitter Agent):\\n{preview}\\n\\n\"\n",
        "                f\"USER QUERY:\\n{user_prompt}\"\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        response = llm.invoke(msg)\n",
        "        out = response.content.strip()\n",
        "        print(\"[final_answer] LLM call completed.\")\n",
        "        return {\"final_answer\": out or \"Final agent returned empty result.\"}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"[final_answer] ERROR:\", str(e))\n",
        "        return {\"final_answer\": f\"Final agent error: {str(e)}\"}\n",
        "\n",
        "\n",
        "\n",
        "# ------------ SCHEDULER ------------\n",
        "def scheduler_node(state: ResearchState):\n",
        "    print(\"\\n[SCHEDULER] Deciding next agent...\")\n",
        "    return {}\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "#               4. BUILD GRAPH\n",
        "# -------------------------------------------------\n",
        "builder = StateGraph(ResearchState)\n",
        "\n",
        "builder.add_node(\"pdf_loader\", pdf_loader_node)\n",
        "builder.add_node(\"scheduler\", scheduler_node)\n",
        "builder.add_node(\"splitter\", splitter_agent_node)\n",
        "builder.add_node(\"planner\", planner_node)\n",
        "builder.add_node(\"final_answer\", final_answer_node)\n",
        "\n",
        "builder.add_edge(START, \"pdf_loader\")\n",
        "builder.add_edge(\"pdf_loader\", \"scheduler\")\n",
        "builder.add_edge(\"scheduler\", \"splitter\")\n",
        "builder.add_edge(\"splitter\", \"planner\")\n",
        "builder.add_edge(\"planner\", \"final_answer\")\n",
        "builder.add_edge(\"final_answer\", END)\n",
        "\n",
        "graph = builder.compile()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6pfe1kPbJQM",
        "outputId": "214043dc-b9d2-41a4-a260-afd062cc5b8c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Google AI Studio API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = input(\"Enter PDF path: \").strip()\n",
        "user_cmd = input(\"Enter your prompt/instruction: \").strip()\n",
        "\n",
        "state: ResearchState = {\n",
        "    \"file_path\": pdf_path,\n",
        "    \"user_prompt\": user_cmd,\n",
        "    \"pdf_pages\": [],\n",
        "    \"split_sections\": {},\n",
        "    \"figures\": \"\",\n",
        "    \"plan\": \"\",\n",
        "    \"final_answer\": \"\"\n",
        "}\n",
        "\n",
        "try:\n",
        "    print(\"\\n\\n===== PIPELINE START =====\")\n",
        "    result = graph.invoke(state)\n",
        "    print(\"===== PIPELINE END =====\\n\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SECTION SPLITTER OUTPUT\")\n",
        "    print(\"=\"*50)\n",
        "    print(result.get(\"split_sections\", {}))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"FINAL ANSWER\")\n",
        "    print(\"=\"*50)\n",
        "    print(result.get(\"final_answer\", \"\"))\n",
        "\n",
        "except Exception:\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTu39n_ukym8",
        "outputId": "bfaf1178-1dcd-4794-b95e-95cbe4c63c85"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter PDF path: /content/Video Depth Anything.pdf\n",
            "Enter your prompt/instruction: What is this paper doing? Tell in 50 words\n",
            "\n",
            "\n",
            "===== PIPELINE START =====\n",
            "\n",
            "[AGENT] pdf_loader running...\n",
            "[pdf_loader] Loaded PDF successfully.\n",
            "\n",
            "[SCHEDULER] Deciding next agent...\n",
            "\n",
            "[AGENT] splitter running...\n",
            "[LLM] Splitter agent calling LLM...\n",
            "[splitter] CLEANED keys: ['Header and Visual Overview', 'Abstract', 'Introduction Monocular Depth Estimation Progress', 'Introduction Limitations of Existing Models', 'Introduction Video Depth Anything Proposed Solution', 'Introduction Experimental Results and Contributions', 'Related Work Monocular Depth Estimation', 'Related Work Consistent Video Depth Estimation', 'Video Depth Anything Overall Method', 'Architecture Overview and Joint Training', 'Architecture Depth Anything V2 Encoder', 'Architecture Spatiotemporal Head', 'Temporal Gradient Matching Loss Introduction']\n",
            "\n",
            "[AGENT] planner running...\n",
            "[LLM] Planner agent calling LLM...\n",
            "[planner] LLM call completed.\n",
            "\n",
            "[AGENT] final_answer running...\n",
            "[LLM] Final answer agent calling LLM...\n",
            "[final_answer] LLM call completed.\n",
            "===== PIPELINE END =====\n",
            "\n",
            "\n",
            "==================================================\n",
            "SECTION SPLITTER OUTPUT\n",
            "==================================================\n",
            "{'Header and Visual Overview': 'arXiv 2501.12375v3 cs.CV 15 Jun 2025 Video Depth Anything Consistent Depth Estimation for Super-Long Videos Sili Chen Hengkai Guo Shengnan Zhu Feihu Zhang Zilong Huang Jiashi Feng Bingyi Kang ByteDance videodepthanything.github.io 3s 22s 42s 59s 79s 99s 122s 140s 161s 175s Figure 1. Left Our model can generate consistent depth predictions for long videos with rich actions. The demo video shows a 196-second 4690 frames long take of pair skating, as sourced from 14 . Right Comparison to baselines in terms of accuracy 1 , consistency, and latency on the Nvidia A100 GPU denoted with circle size . Consistency is defined as the maximum Temporal Alignment Error TAE among all models minus the TAE of each individual model. Our model achieves the best performance in all aspects.', 'Abstract': 'Depth Anything has achieved remarkable success in monocular depth estimation with strong generalization abil- ity. However, it suffers from temporal inconsistency in videos, hindering its practical applications. Various methods have been proposed to alleviate this issue by leveraging video gen- eration models or introducing priors from optical flow and camera poses. Nonetheless, these methods are only applica- ble to short videos 10 seconds and require a trade-off between quality and computational efficiency. We propose Video Depth Anything for high-quality, consistent depth esti- mation in super-long videos over several minutes without sacrificing efficiency. We base our model on Depth Anything V2 and replace its head with an efficient spatial-temporal head. We design a straightforward yet effective temporal consistency loss by constraining the temporal depth gradient, Corresponding author. eliminating the need for additional geometric priors. The model is trained on a joint dataset of video depth and unla- beled images, similar to Depth Anything V2. Moreover, a novel key-frame-based strategy is developed for long video inference. Experiments show that our model can be applied to arbitrarily long videos without compromising quality, con- sistency, or generalization ability. Comprehensive evalua- tions on multiple video benchmarks demonstrate that our approach sets a new state-of-the-art in zero-shot video depth estimation. We offer models of different scales to support a range of scenarios, with our smallest model capable of real-time performance at 30 FPS.', 'Introduction Monocular Depth Estimation Progress': 'Recently, monocular depth estimation MDE has made sig- nificant progress, as evidenced by advances in depth foun- dation models 3, 17, 42, 43 . For example, Depth Any- thing V2 43 demonstrates a strong generalization ability in producing depth predictions with rich details in various scenarios while being computationally efficient.', 'Introduction Limitations of Existing Models': 'However, these models have a major limitation they are mainly de- signed for static images and suffer from flickering and mo- tion blur in videos. This limitation restricts their applications in robotics 8 , augmented reality 12 , and advanced video editing 25, 47 , which requires temporally consistent depth. Extensive efforts are being made to address this problem. Early work 18, 21, 48 often relies on test-time optimization to tune a pretrained monocular depth model with sophisti- cated geometry constraints. Given the heavy overhead at inference time of these methods, recent work mainly fo- cuses on feedforward models and can be categorized into two approaches. The first approach 39 involves design- ing a plug-and-play module to augment monocular depth model predictions with temporal consistency. The training of such a module is highly dependent on optical flow 40 or camera poses 30 for consistency constraints, making the module susceptible to corresponding errors. The second approach 13, 32, 41 repurposes pre-trained video diffusion models 4 into video-to-depth models. These methods excel at producing fine-grained details, but are computationally in- efficient, cannot leverage existing depth foundation models, and can only handle videos with limited length.', 'Introduction Video Depth Anything Proposed Solution': 'Then, a natural question arises Is it possible to have a model that can perfectly inherit the capabilities of existing foundation models while achieving temporal stability for arbitrarily long videos In this paper, we show that the answer is YES by developing Video Depth Anythingbased on Depth Anything V2, without sacrificing its generalization ability, richness in details, or computational efficiency. This target is achieved without introducing any geometric priors or video generation priors. Specifically, we first design a lightweight spatial-temporal head STH to replace the DPT head 28 and enable tem- poral information interactions. STH includes four temporal attention layers, applied along the temporal dimension for each spatial position. Introducing temporal attention only in the head prevents the learned representation from being corrupted by the limited video data. Then, we propose a temporal gradient matching loss to constrain depth predic- tion gradients along the temporal dimension to match those calculated from the ground truth. This loss function is jointly optimized with the scale-shift-invariant loss and spatial gra- dient matching loss 3, 42 . Despite its simplicity, it can effectively boost the model s temporal consistency. Third, to maintain the original capabilities of the model, we train it jointly on 730K video frames with depth annotations using supervised learning, and on 0.62 million unlabeled images using self-training, similar to Depth Anything V2 43 . To handle super-long videos at inference time, we developed a novel segment-wise processing strategy. Each new segment is concatenated with eight overlapping frames and two key frames from the previous video clips, forming a total of 32 frames. Then, the overlapping frames will be progressively interpolated between the two consecutive windows to ensure smoothness.', 'Introduction Experimental Results and Contributions': 'We compare our model with baselines on five datasets for zero-shot video depth estimation. Our model achieves state-of-the-art SOTA results on four of the datasets in terms of spatial accuracy and outperforms all baselines on all datasets in terms of temporal consistency. Not only can our model produce depth outputs visually comparable to video-diffusion-based methods, but it is also significantly more computationally efficient. For the first time, we can es- timate consistent depth for videos over several minutes see Fig. 1 . Additionally, we tested our model for zero-shot im- age depth estimation on five datasets, noting only a marginal performance drop on one dataset compared to Depth Any- thing V2. As shown in Fig. 1 right , our model achieves the best performance in all three aspects spatial accuracy, temporal consistency, and computational efficiency. Our contributions are summarized as follows We develop a novel method to transform Depth Anything into Video Depth Anythingfor depth estimation in arbitrar- ily long videos. We propose a simple yet effective loss function that en- forces temporal consistency constraints without introduc- ing geometric or generative priors. Our model not only sets new SOTA both spatially and temporally in video depth estimation but is also the most computationally efficient.', 'Related Work Monocular Depth Estimation': 'Monocular depth estimation.Early monocular depth es- timation 1, 9, 10, 46 efforts were primarily trained and tested on in-domain datasets. These models were constrained by the limited diversity of their training data, showing bad generalization for zero-shot application. Subsequently, Mi- DaS 3 introduced multi-dataset mixed training using an affine-invariant alignment method, significantly enhancing model generalization. However, due to limitations in the backbone model s performance and noise in the labeled depth data, the resulting depth maps lacked fine details. Following MiDaS 3 , monocular depth estimation models have generally bifurcated into two categories relative depth models that estimate affine-invariant depth e.g., DPT 28 , DepthAnything 42, 43 , Marigold 17 and metric depth models that estimate depth with an absolute scale e.g., ZoeDepth 2 , Metric3D 45 , UniDepth 27 . Metric depth models require training with metric depth data that includes camera parameters 45 , thus their available training data is more limited compared to affine-invariant depth models, resulting in poorer generalization. Recently, Depth Anything V2 43 leveraged the Dinov2 23 pre-trained backbone to train an affine-invariant large-scale model using synthetic data with high-detail fidelity. This large model was then used as a teacher to distill smaller models on 62 million un- labeled datasets 42 , achieving SOTA performance in both generalization and geometric details. However, since Depth Anything V2 43 was trained exclusively on static images, thus lacks temporal consistency.', 'Related Work Consistent Video Depth Estimation': 'Consistent video depth estimation.The core task of consistent video depth estimation is to obtain temporal con- sistent and accuracy depth maps. Early methods for video depth relied on test-time training 18, 21, 48 , which were impractical for applications for their low efficiency. In re- cent years, learning-based models have emerged. Some of these models, such as MAMo 44 , use optical flow 33 to warp features, while others 29 depends on relative poses between frames to construct cost volumes. However, their performance were suffered from errors of inaccurate opti- cal flow or pose estimation. Additional approaches have attempted to enhance off-the-shelf monocular depth estima- tion MDE models with temporal stability model blocks 39 . Nevertheless, these efforts have not achieved satisfac- tory results due to suboptimal model designs and inadequate geometric consistency constraints. Furthermore, video diffu- sion models such as ChronoDepth 32 , DepthCrafter 13 , and DepthAnyVideo 41 show better details and temporal consistency. But they suffered from slow inference speeds and require extensive video depth training data. Limited by the large memory, these models 13 were typically tested only within the maximum window length used during train- ing, leading to depth flickering between windows and poor temporal and spatial consistency in long videos.', 'Video Depth Anything Overall Method': 'In this section, we introduce Video Depth Anything, a feed- forward video transformer model to efficiently estimate tem- porally consistent video depth. We adopt the affine-invariant depth, but share the same scale and shift across the entire video. The pipeline of our method is shown in Fig. 2. Our model is built upon Depth Anything V2 with an additional temporal module and video dataset training Sec. 3.1 . A novel loss to enfoce temporal consistency is proposed in Sec. 3.2. Finally, a strategy combined with overlapping frames and key frames is presented to efficiently support super-long video inference Sect. 3.3 .', 'Architecture Overview and Joint Training': 'Due to the lack of sufficient video depth data, we start with a pre-trained image depth estimation model, Depth Anything V2, and adopt a joint training strategy using both image and video data.', 'Architecture Depth Anything V2 Encoder': 'Depth Anything V2 Encoder.Depth Anything V2 43 is the current state-of-the-art monocular depth estimation model, characterized by its high accuracy and generalization capabilities. We use its trained model as our encoder. To reduce training costs and preserve well-learned features, the encoder is frozen during training. Unlike monocular depth encoders that only accept image input, our training scenario requires the encoder to process simultaneously both video and image data. To extract fea- tures from video frames with an image encoder, we collapse the temporal dimension of a video clip into the batch dimen- sion. The input data are denoted as X R B N C H W , where B represents the batch size,N is the number of frames in the video clip, N 1 for the image as input, C, H, W are the number of channels, height, width of the frames, re- spectively. The encoder takes X as input to produce a series of intermediate feature maps Fi R B N H p W p Ci, p is the patch size of the encoder. Although the image en- coder extracts strong visual representations from individual frames, it neglects the temporal information interactions be- tween frames. Thus, the spatiotemporal head is introduced to model the temporal relationship among the frames.', 'Architecture Spatiotemporal Head': 'Spatiotemporal Head.The spatiotemporal head STH is built upon the DPT 28 head and with the only modifica- tion being the insertion of temporal layers to capture tempo- ral information. A temporal layer consists of a multi-head self-attention 34 model SA and a feed-forward network FFN . When inputting a feature Fi into the temporal layer, the temporal dimension N is isolated, and self-attention is executed solely along the temporal dimension to facilitate the interaction of temporal features. To capture temporal positional relationships among different frames, we utilize absolute positional embedding to encode temporal positional information from the video sequence. The spatiotemporal head uniformly samples 4 feature maps from Fi including the final features from the encoder, denoted as F4 as inputs, and predicts a depth map D RH W . As shown in Figure 2, the selected features Fi are fed into the Reassemble layer to produce a feature pyramid. Then, the features are gradually fused from low resolution to high resolution by the Fusion layer. The Reassemble and Fusion layer are proposed by DPT 28 . The final fused high-resolution feature maps are passed through the output layer to produce the depth map D. To reduce the additional computational load, we insert the temporal layer at a few positions with lower feature resolutions.', 'Temporal Gradient Matching Loss Introduction': 'In this section, we start with the Optical Flow Based Warping OPW loss, then explore new loss designs and ultimately propose a Temporal Gradient Matching Loss TGM that does not rely on optical flow, yet still ensures the temporal consistency of predictions between frames. OPW loss. To constrain temporal consistency, previous video models such as 19, 38, 39 assume that the depths at corresponding positions in adjacent frames, identified through optical flow, are consistent, e.g., the Optical Flow based Warping OPW loss proposed in NVDS 39 . OPW loss is computed after obtaining corresponding points on the basis of optical flow and warpi'}\n",
            "\n",
            "==================================================\n",
            "FINAL ANSWER\n",
            "==================================================\n",
            "1. This paper proposes Video Depth Anything, an extension of Depth Anything V2, for high-quality, temporally consistent depth estimation in super-long videos. It replaces the original head with an efficient spatial-temporal head and employs a novel temporal consistency loss. This enables consistent predictions for arbitrarily long videos, achieving state-of-the-art consistency, quality, and efficiency.\n",
            "(Content extracted from 'Abstract')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/Video Depth Anything.pdf\"\n",
        "user_cmd = \"Tell me the overall summary of the paper\"\n",
        "\n",
        "state: ResearchState = {\n",
        "    \"file_path\": pdf_path,\n",
        "    \"user_prompt\": user_cmd,\n",
        "    \"pdf_pages\": [],\n",
        "    \"split_sections\": {},\n",
        "    \"figures\": \"\",\n",
        "    \"plan\": \"\",\n",
        "    \"final_answer\": \"\"\n",
        "}\n",
        "\n",
        "try:\n",
        "    print(\"\\n\\n===== PIPELINE START =====\")\n",
        "    result = graph.invoke(state)\n",
        "    print(\"===== PIPELINE END =====\\n\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SECTION SPLITTER OUTPUT\")\n",
        "    print(\"=\"*50)\n",
        "    print(result.get(\"split_sections\", {}))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"FINAL ANSWER\")\n",
        "    print(\"=\"*50)\n",
        "    print(result.get(\"final_answer\", \"\"))\n",
        "\n",
        "except Exception:\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnmRNxBXY5QU",
        "outputId": "844ae53b-a24c-49e7-f718-d80fb68ba6ba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "===== PIPELINE START =====\n",
            "\n",
            "[AGENT] pdf_loader running...\n",
            "[pdf_loader] Loaded PDF successfully.\n",
            "\n",
            "[SCHEDULER] Deciding next agent...\n",
            "\n",
            "[AGENT] splitter running...\n",
            "[LLM] Splitter agent calling LLM...\n",
            "[splitter] CLEANED keys: ['ArXiv Info and Figure 1', 'Abstract', 'Introduction Monocular Depth Estimation Progress', 'Introduction Existing Approaches and Limitations', 'Introduction Video Depth Anything Core Idea', 'Introduction Technical Design of Video Depth Anything', 'Introduction Experimental Results and Contributions Summary', 'Related Work Monocular Depth Estimation Overview', 'Related Work Consistent Video Depth Estimation Overview', 'Video Depth Anything Model Introduction', 'Architecture Overview and Training Strategy', 'Architecture Depth Anything V2 Encoder Details', 'Architecture Spatiotemporal Head STH Details', 'Temporal Gradient Matching Loss Introduction', 'Temporal Gradient Matching Loss OPW Description']\n",
            "\n",
            "[AGENT] planner running...\n",
            "[LLM] Planner agent calling LLM...\n",
            "[planner] LLM call completed.\n",
            "\n",
            "[AGENT] final_answer running...\n",
            "[LLM] Final answer agent calling LLM...\n",
            "[final_answer] LLM call completed.\n",
            "===== PIPELINE END =====\n",
            "\n",
            "\n",
            "==================================================\n",
            "SECTION SPLITTER OUTPUT\n",
            "==================================================\n",
            "{'ArXiv Info and Figure 1': 'arXiv 2501.12375v3 cs.CV 15 Jun 2025 Video Depth Anything Consistent Depth Estimation for Super-Long Videos Sili Chen Hengkai Guo Shengnan Zhu Feihu Zhang Zilong Huang Jiashi Feng Bingyi Kang ByteDance videodepthanything.github.io 3s 22s 42s 59s 79s 99s 122s 140s 161s 175s Figure 1. Left Our model can generate consistent depth predictions for long videos with rich actions. The demo video shows a 196-second 4690 frames long take of pair skating, as sourced from 14 . Right Comparison to baselines in terms of accuracy 1 , consistency, and latency on the Nvidia A100 GPU denoted with circle size . Consistency is defined as the maximum Temporal Alignment Error TAE among all models minus the TAE of each individual model. Our model achieves the best performance in all aspects.', 'Abstract': 'Depth Anything has achieved remarkable success in monocular depth estimation with strong generalization ability. However, it suffers from temporal inconsistency in videos, hindering its practical applications. Various methods have been proposed to alleviate this issue by leveraging video generation models or introducing priors from optical flow and camera poses. Nonetheless, these methods are only applicable to short videos 10 seconds and require a trade-off between quality and computational efficiency. We propose Video Depth Anything for high-quality, consistent depth estimation in super-long videos over several minutes without sacrificing efficiency. We base our model on Depth Anything V2 and replace its head with an efficient spatial-temporal head. We design a straightforward yet effective temporal consistency loss by constraining the temporal depth gradient, Corresponding author. eliminating the need for additional geometric priors. The model is trained on a joint dataset of video depth and unlabeled images, similar to Depth Anything V2. Moreover, a novel key-frame-based strategy is developed for long video inference. Experiments show that our model can be applied to arbitrarily long videos without compromising quality, consistency, or generalization ability. Comprehensive evaluations on multiple video benchmarks demonstrate that our approach sets a new state-of-the-art in zero-shot video depth estimation. We offer models of different scales to support a range of scenarios, with our smallest model capable of real-time performance at 30 FPS.', 'Introduction Monocular Depth Estimation Progress': '1. Introduction Recently, monocular depth estimation MDE has made significant progress, as evidenced by advances in depth foundation models 3, 17, 42, 43 . For example, Depth Anything V2 43 demonstrates a strong generalization ability in producing depth predictions with rich details in various scenarios while being computationally efficient. However, these models have a major limitation they are mainly designed for static images and suffer from flickering and motion blur in videos. This limitation restricts their applications in robotics 8 , augmented reality 12 , and advanced video editing 25, 47 , which requires temporally consistent depth.', 'Introduction Existing Approaches and Limitations': 'Extensive efforts are being made to address this problem. Early work 18, 21, 48 often relies on test-time optimization to tune a pretrained monocular depth model with sophisticated geometry constraints. Given the heavy overhead at inference time of these methods, recent work mainly focuses on feedforward models and can be categorized into two approaches. The first approach 39 involves designing a plug-and-play module to augment monocular depth model predictions with temporal consistency. The training of such a module is highly dependent on optical flow 40 or camera poses 30 for consistency constraints, making the module susceptible to corresponding errors. The second approach 13, 32, 41 repurposes pre-trained video diffusion models 4 into video-to-depth models. These methods excel at producing fine-grained details, but are computationally inefficient, cannot leverage existing depth foundation models, and can only handle videos with limited length.', 'Introduction Video Depth Anything Core Idea': 'Then, a natural question arises Is it possible to have a model that can perfectly inherit the capabilities of existing foundation models while achieving temporal stability for arbitrarily long videos In this paper, we show that the answer is YES by developing Video Depth Anythingbased on Depth Anything V2, without sacrificing its generalization ability, richness in details, or computational efficiency. This target is achieved without introducing any geometric priors or video generation priors.', 'Introduction Technical Design of Video Depth Anything': 'Specifically, we first design a lightweight spatial-temporal head STH to replace the DPT head 28 and enable temporal information interactions. STH includes four temporal attention layers, applied along the temporal dimension for each spatial position. Introducing temporal attention only in the head prevents the learned representation from being corrupted by the limited video data. Then, we propose a temporal gradient matching loss to constrain depth prediction gradients along the temporal dimension to match those calculated from the ground truth. This loss function is jointly optimized with the scale-shift-invariant loss and spatial gradient matching loss 3, 42 . Despite its simplicity, it can effectively boost the model s temporal consistency. Third, to maintain the original capabilities of the model, we train it jointly on 730K video frames with depth annotations using supervised learning, and on 0.62 million unlabeled images using self-training, similar to Depth Anything V2 43 . To handle super-long videos at inference time, we developed a novel segment-wise processing strategy. Each new segment is concatenated with eight overlapping frames and two key frames from the previous video clips, forming a total of 32 frames. Then, the overlapping frames will be progressively interpolated between the two consecutive windows to ensure smoothness.', 'Introduction Experimental Results and Contributions Summary': 'We compare our model with baselines on five datasets for zero-shot video depth estimation. Our model achieves state-of-the-art SOTA results on four of the datasets in terms of spatial accuracy and outperforms all baselines on all datasets in terms of temporal consistency. Not only can our model produce depth outputs visually comparable to video-diffusion-based methods, but it is also significantly more computationally efficient. For the first time, we can estimate consistent depth for videos over several minutes see Fig. 1 . Additionally, we tested our model for zero-shot image depth estimation on five datasets, noting only a marginal performance drop on one dataset compared to Depth Anything V2. As shown in Fig. 1 right , our model achieves the best performance in all three aspects spatial accuracy, temporal consistency, and computational efficiency. Our contributions are summarized as follows We develop a novel method to transform Depth Anything into Video Depth Anythingfor depth estimation in arbitrarily long videos. We propose a simple yet effective loss function that enforces temporal consistency constraints without introducing geometric or generative priors. Our model not only sets new SOTA both spatially and temporally in video depth estimation but is also the most computationally efficient.', 'Related Work Monocular Depth Estimation Overview': '2. Related Work Monocular depth estimation.Early monocular depth estimation 1, 9, 10, 46 efforts were primarily trained and tested on in-domain datasets. These models were constrained by the limited diversity of their training data, showing bad generalization for zero-shot application. Subsequently, MiDaS 3 introduced multi-dataset mixed training using an affine-invariant alignment method, significantly enhancing model generalization. However, due to limitations in the backbone model s performance and noise in the labeled depth data, the resulting depth maps lacked fine details. Following MiDaS 3 , monocular depth estimation models have generally bifurcated into two categories relative depth models that estimate affine-invariant depth e.g., DPT 28 , DepthAnything 42, 43 , Marigold 17 and metric depth models that estimate depth with an absolute scale e.g., ZoeDepth 2 , Metric3D 45 , UniDepth 27 . Metric depth models require training with metric depth data that includes camera parameters 45 , thus their available training data is more limited compared to affine-invariant depth models, resulting in poorer generalization. Recently, Depth Anything V2 43 leveraged the Dinov2 23 pre-trained backbone to train an affine-invariant large-scale model using synthetic data with high-detail fidelity. This large model was then used as a teacher to distill smaller models on 62 million unlabeled datasets 42 , achieving SOTA performance in both generalization and geometric details. However, since Depth Anything V2 43 was trained exclusively on static images, thus lacks temporal consistency.', 'Related Work Consistent Video Depth Estimation Overview': 'Consistent video depth estimation.The core task of consistent video depth estimation is to obtain temporal consistent and accuracy depth maps. Early methods for video depth relied on test-time training 18, 21, 48 , which were impractical for applications for their low efficiency. In recent years, learning-based models have emerged. Some of these models, such as MAMo 44 , use optical flow 33 to warp features, while others 29 depends on relative poses between frames to construct cost volumes. However, their performance were suffered from errors of inaccurate optical flow or pose estimation. Additional approaches have attempted to enhance off-the-shelf monocular depth estimation MDE models with temporal stability model blocks 39 . Nevertheless, these efforts have not achieved satisfactory results due to suboptimal model designs and inadequate geometric consistency constraints. Furthermore, video diffusion models such as ChronoDepth 32 , DepthCrafter 13 , and DepthAnyVideo 41 show better details and temporal consistency. But they suffered from slow inference speeds and require extensive video depth training data. Limited by the large memory, these models 13 were typically tested only within the maximum window length used during training, leading to depth flickering between windows and poor temporal and spatial consistency in long videos.', 'Video Depth Anything Model Introduction': '3. Video Depth Anything In this section, we introduce Video Depth Anything, a feed-forward video transformer model to efficiently estimate temporally consistent video depth. We adopt the affine-invariant depth, but share the same scale and shift across the entire video. The pipeline of our method is shown in Fig. 2. Our model is built upon Depth Anything V2 with an additional temporal module and video dataset training Sec. 3.1 . A novel loss to enfoce temporal consistency is proposed in Sec. 3.2. Finally, a strategy combined with overlapping frames and key frames is presented to efficiently support super-long video inference Sect. 3.3 .', 'Architecture Overview and Training Strategy': '3.1. Architecture Due to the lack of sufficient video depth data, we start with a pre-trained image depth estimation model, Depth Anything V2, and adopt a joint training strategy using both image and video data.', 'Architecture Depth Anything V2 Encoder Details': 'Depth Anything V2 Encoder.Depth Anything V2 43 is the current state-of-the-art monocular depth estimation model, characterized by its high accuracy and generalization capabilities. We use its trained model as our encoder. To reduce training costs and preserve well-learned features, the encoder is frozen during training. Unlike monocular depth encoders that only accept image input, our training scenario requires the encoder to process simultaneously both video and image data. To extract features from video frames with an image encoder, we collapse the temporal dimension of a video clip into the batch dimension. The input data are denoted as X R B N C H W , where B represents the batch size,N is the number of frames in the video clip, N 1 for the image as input, C, H, W are the number of channels, height, width of the frames, respectively. The encoder takes X as input to produce a series of intermediate feature maps Fi R B N H p W p Ci, p is the patch size of the encoder. Although the image encoder extracts strong visual representations from individual frames, it neglects the temporal information interactions between frames. Thus, the spatiotemporal head is introduced to model the temporal relationship among the frames.', 'Architecture Spatiotemporal Head STH Details': 'Spatiotemporal Head.The spatiotemporal head STH is built upon the DPT 28 head and with the only modification being the insertion of temporal layers to capture temporal information. A temporal layer consists of a multi-head self-attention 34 model SA and a feed-forward network FFN . When inputting a feature Fi into the temporal layer, the temporal dimension N is isolated, and self-attention is executed solely along the temporal dimension to facilitate the interaction of temporal features. To capture temporal positional relationships among different frames, we utilize absolute positional embedding to encode temporal positional information from the video sequence. The spatiotemporal head uniformly samples 4 feature maps from Fi including the final features from the encoder, denoted as F4 as inputs, and predicts a depth map D RH W . As shown in Figure 2, the selected features Fi are fed into the Reassemble layer to produce a feature pyramid. Then, the features are gradually fused from low resolution to high resolution by the Fusion layer. The Reassemble and Fusion layer are proposed by DPT 28 . The final fused high-resolution feature maps are passed through the output layer to produce the depth map D. To reduce the additional computational load, we insert the temporal layer at a few positions with lower feature resolutions.', 'Temporal Gradient Matching Loss Introduction': '3.2. Temporal Gradient Matching loss In this section, we start with the Optical Flow Based Warping OPW loss, then explore new loss designs and ultimately propose a Temporal Gradient Matching Loss TGM that does not rely on optical flow, yet still ensures the temporal consistency of predictions between frames.', 'Temporal Gradient Matching Loss OPW Description': 'OPW loss. To constrain temporal consistency, previous video models such as 19, 38, 39 assume that the depths at corresponding positions in adjacent frames, identified through optical flow, are consistent, e.g., the Optical Flow based Warping OPW loss proposed in NVDS 39 . OPW loss is computed after obtaining corresponding points on the basis of optical flow and warpi'}\n",
            "\n",
            "==================================================\n",
            "FINAL ANSWER\n",
            "==================================================\n",
            "The paper introduces Video Depth Anything, a novel model designed to address the temporal inconsistency issues of monocular depth estimation models, such as Depth Anything, when applied to videos. Existing video depth methods are typically limited to short videos (under 10 seconds), require trade-offs between quality and computational efficiency, or depend on external priors like optical flow or camera poses.\n",
            "\n",
            "Video Depth Anything overcomes these limitations by building upon Depth Anything V2 and incorporating several key innovations:\n",
            "1.  **Efficient Spatial-Temporal Head (STH)**: It replaces Depth Anything V2's original head with an STH that includes temporal attention layers, enabling temporal information interactions without corrupting learned representations.\n",
            "2.  **Temporal Gradient Matching Loss**: A simple yet effective loss function is introduced to constrain temporal depth gradients, ensuring consistency between frames without relying on additional geometric or generative priors.\n",
            "3.  **Joint Training Strategy**: The model is trained on a combined dataset of video frames with depth annotations and unlabeled images, similar to Depth Anything V2, to maintain its strong generalization ability.\n",
            "4.  **Key-Frame-Based Inference Strategy**: A novel segment-wise processing strategy, utilizing overlapping frames and key frames, is developed to efficiently handle super-long videos (up to several minutes) at inference time.\n",
            "\n",
            "Experimental results demonstrate that Video Depth Anything achieves state-of-the-art performance in both spatial accuracy and temporal consistency across multiple datasets. It is also significantly more computationally efficient than other methods and is the first model capable of consistently estimating depth for videos exceeding several minutes in length, while preserving the high quality and generalization ability of its base model.\n",
            "\n",
            "1.  **Problem Statement**: Monocular depth estimation models, while effective for static images, suffer from temporal inconsistency (flickering, motion blur) when applied to videos, hindering practical applications like robotics and video editing. Existing solutions are often limited to short videos, are computationally inefficient, or rely on potentially error-prone geometric priors.\n",
            "    *   *Cited subsections: 'Abstract', 'Introduction Monocular Depth Estimation Progress', 'Introduction Existing Approaches and Limitations'*\n",
            "2.  **Proposed Solution - Video Depth Anything**: The paper introduces Video Depth Anything, a feed-forward video transformer model based on Depth Anything V2, designed to provide high-quality, consistent depth estimation for super-long videos (several minutes) without sacrificing efficiency or generalization ability.\n",
            "    *   *Cited subsections: 'Abstract', 'Introduction Video Depth Anything Core Idea', 'Video Depth Anything Model Introduction'*\n",
            "3.  **Key Technical Innovations**: The model achieves its goals by replacing Depth Anything V2's head with an efficient spatial-temporal head (STH) that integrates temporal attention, and by introducing a straightforward yet effective temporal gradient matching loss to enforce consistency without external geometric or generative priors. It also utilizes a joint training strategy on video depth and unlabeled images, and a key-frame-based inference strategy for super-long videos.\n",
            "    *   *Cited subsections: 'Abstract', 'Introduction Technical Design of Video Depth Anything', 'Architecture Spatiotemporal Head STH Details', 'Temporal Gradient Matching Loss Introduction', 'Architecture Overview and Training Strategy'*\n",
            "4.  **Performance and Contributions**: Video Depth Anything achieves state-of-the-art performance in both spatial accuracy and temporal consistency across various benchmarks, significantly outperforming baselines in computational efficiency. It is the first model capable of consistently estimating depth for videos over several minutes, demonstrating visually comparable outputs to video-diffusion-based methods while being more efficient. The model also retains the strong generalization ability of Depth Anything V2.\n",
            "    *   *Cited subsections: 'Abstract', 'ArXiv Info and Figure 1', 'Introduction Experimental Results and Contributions Summary'*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/Video Depth Anything.pdf\"\n",
        "user_cmd = \"What is the objective of this paper?\"\n",
        "\n",
        "state: ResearchState = {\n",
        "    \"file_path\": pdf_path,\n",
        "    \"user_prompt\": user_cmd,\n",
        "    \"pdf_pages\": [],\n",
        "    \"split_sections\": {},\n",
        "    \"figures\": \"\",\n",
        "    \"plan\": \"\",\n",
        "    \"final_answer\": \"\"\n",
        "}\n",
        "\n",
        "try:\n",
        "    print(\"\\n\\n===== PIPELINE START =====\")\n",
        "    result = graph.invoke(state)\n",
        "    print(\"===== PIPELINE END =====\\n\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SECTION SPLITTER OUTPUT\")\n",
        "    print(\"=\"*50)\n",
        "    print(result.get(\"split_sections\", {}))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"FINAL ANSWER\")\n",
        "    print(\"=\"*50)\n",
        "    print(result.get(\"final_answer\", \"\"))\n",
        "\n",
        "except Exception:\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2MCJJhit4Z3",
        "outputId": "146d199d-a2fc-4901-b4d5-51f873cf84b0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "===== PIPELINE START =====\n",
            "\n",
            "[AGENT] pdf_loader running...\n",
            "[pdf_loader] Loaded PDF successfully.\n",
            "\n",
            "[SCHEDULER] Deciding next agent...\n",
            "\n",
            "[AGENT] splitter running...\n",
            "[LLM] Splitter agent calling LLM...\n",
            "[splitter] CLEANED keys: ['Title and Authors', 'Figure 1 Description', 'Abstract', 'Introduction Background and Problem', 'Introduction Existing Approaches', 'Introduction Our Solution', 'Introduction Methodology Highlights', 'Introduction Experimental Summary', 'Contributions', 'Related Work Monocular Depth Estimation', 'Related Work Consistent Video Depth Estimation', 'Video Depth Anything Overview', 'Video Depth Anything Architecture Encoder', 'Video Depth Anything Architecture Spatiotemporal Head', 'Video Depth Anything Temporal Gradient Matching Loss']\n",
            "\n",
            "[AGENT] planner running...\n",
            "[LLM] Planner agent calling LLM...\n",
            "[planner] LLM call completed.\n",
            "\n",
            "[AGENT] final_answer running...\n",
            "[LLM] Final answer agent calling LLM...\n",
            "[final_answer] LLM call completed.\n",
            "===== PIPELINE END =====\n",
            "\n",
            "\n",
            "==================================================\n",
            "SECTION SPLITTER OUTPUT\n",
            "==================================================\n",
            "{'Title and Authors': 'arXiv 2501.12375v3 cs.CV 15 Jun 2025 Video Depth Anything Consistent Depth Estimation for Super-Long Videos Sili Chen Hengkai Guo Shengnan Zhu Feihu Zhang Zilong Huang Jiashi Feng Bingyi Kang ByteDance videodepthanything.github.io', 'Figure 1 Description': 'Figure 1. Left Our model can generate consistent depth predictions for long videos with rich actions. The demo video shows a 196-second 4690 frames long take of pair skating, as sourced from 14 . Right Comparison to baselines in terms of accuracy 1 , consistency, and latency on the Nvidia A100 GPU denoted with circle size . Consistency is defined as the maximum Temporal Alignment Error TAE among all models minus the TAE of each individual model. Our model achieves the best performance in all aspects.', 'Abstract': 'Depth Anything has achieved remarkable success in monocular depth estimation with strong generalization abil- ity. However, it suffers from temporal inconsistency in videos, hindering its practical applications. Various methods have been proposed to alleviate this issue by leveraging video gen- eration models or introducing priors from optical flow and camera poses. Nonetheless, these methods are only applica- ble to short videos 10 seconds and require a trade-off between quality and computational efficiency. We propose Video Depth Anything for high-quality, consistent depth esti- mation in super-long videos over several minutes without sacrificing efficiency. We base our model on Depth Anything V2 and replace its head with an efficient spatial-temporal head. We design a straightforward yet effective temporal consistency loss by constraining the temporal depth gradient, Corresponding author. eliminating the need for additional geometric priors. The model is trained on a joint dataset of video depth and unla- beled images, similar to Depth Anything V2. Moreover, a novel key-frame-based strategy is developed for long video inference. Experiments show that our model can be applied to arbitrarily long videos without compromising quality, con- sistency, or generalization ability. Comprehensive evalua- tions on multiple video benchmarks demonstrate that our approach sets a new state-of-the-art in zero-shot video depth estimation. We offer models of different scales to support a range of scenarios, with our smallest model capable of real-time performance at 30 FPS.', 'Introduction Background and Problem': '1. Introduction Recently, monocular depth estimation MDE has made sig- nificant progress, as evidenced by advances in depth foun- dation models 3, 17, 42, 43 . For example, Depth Any- thing V2 43 demonstrates a strong generalization ability in producing depth predictions with rich details in various scenarios while being computationally efficient. However, these models have a major limitation they are mainly de- signed for static images and suffer from flickering and mo- tion blur in videos. This limitation restricts their applications in robotics 8 , augmented reality 12 , and advanced video editing 25, 47 , which requires temporally consistent depth.', 'Introduction Existing Approaches': 'Extensive efforts are being made to address this problem. Early work 18, 21, 48 often relies on test-time optimization to tune a pretrained monocular depth model with sophisti- cated geometry constraints. Given the heavy overhead at inference time of these methods, recent work mainly fo- cuses on feedforward models and can be categorized into two approaches. The first approach 39 involves design- ing a plug-and-play module to augment monocular depth model predictions with temporal consistency. The training of such a module is highly dependent on optical flow 40 or camera poses 30 for consistency constraints, making the module susceptible to corresponding errors. The second approach 13, 32, 41 repurposes pre-trained video diffusion models 4 into video-to-depth models. These methods excel at producing fine-grained details, but are computationally in- efficient, cannot leverage existing depth foundation models, and can only handle videos with limited length.', 'Introduction Our Solution': 'Then, a natural question arises Is it possible to have a model that can perfectly inherit the capabilities of existing foundation models while achieving temporal stability for arbitrarily long videos In this paper, we show that the answer is YES by developing Video Depth Anythingbased on Depth Anything V2, without sacrificing its generalization ability, richness in details, or computational efficiency. This target is achieved without introducing any geometric priors or video generation priors.', 'Introduction Methodology Highlights': 'Specifically, we first design a lightweight spatial-temporal head STH to replace the DPT head 28 and enable tem- poral information interactions. STH includes four temporal attention layers, applied along the temporal dimension for each spatial position. Introducing temporal attention only in the head prevents the learned representation from being corrupted by the limited video data. Then, we propose a temporal gradient matching loss to constrain depth predic- tion gradients along the temporal dimension to match those calculated from the ground truth. This loss function is jointly optimized with the scale-shift-invariant loss and spatial gra- dient matching loss 3, 42 . Despite its simplicity, it can effectively boost the model s temporal consistency. Third, to maintain the original capabilities of the model, we train it jointly on 730K video frames with depth annotations using supervised learning, and on 0.62 million unlabeled images using self-training, similar to Depth Anything V2 43 . To handle super-long videos at inference time, we developed a novel segment-wise processing strategy. Each new segment is concatenated with eight overlapping frames and two key frames from the previous video clips, forming a total of 32 frames. Then, the overlapping frames will be progressively interpolated between the two consecutive windows to ensure smoothness.', 'Introduction Experimental Summary': 'We compare our model with baselines on five datasets for zero-shot video depth estimation. Our model achieves state-of-the-art SOTA results on four of the datasets in terms of spatial accuracy and outperforms all baselines on all datasets in terms of temporal consistency. Not only can our model produce depth outputs visually comparable to video-diffusion-based methods, but it is also significantly more computationally efficient. For the first time, we can es- timate consistent depth for videos over several minutes see Fig. 1 . Additionally, we tested our model for zero-shot im- age depth estimation on five datasets, noting only a marginal performance drop on one dataset compared to Depth Any- thing V2. As shown in Fig. 1 right , our model achieves the best performance in all three aspects spatial accuracy, temporal consistency, and computational efficiency.', 'Contributions': 'Our contributions are summarized as follows We develop a novel method to transform Depth Anything into Video Depth Anythingfor depth estimation in arbitrar- ily long videos. We propose a simple yet effective loss function that en- forces temporal consistency constraints without introduc- ing geometric or generative priors. Our model not only sets new SOTA both spatially and temporally in video depth estimation but is also the most computationally efficient.', 'Related Work Monocular Depth Estimation': '2. Related Work Monocular depth estimation. Early monocular depth es- timation 1, 9, 10, 46 efforts were primarily trained and tested on in-domain datasets. These models were constrained by the limited diversity of their training data, showing bad generalization for zero-shot application. Subsequently, Mi- DaS 3 introduced multi-dataset mixed training using an affine-invariant alignment method, significantly enhancing model generalization. However, due to limitations in the backbone model s performance and noise in the labeled depth data, the resulting depth maps lacked fine details. Following MiDaS 3 , monocular depth estimation models have generally bifurcated into two categories relative depth models that estimate affine-invariant depth e.g., DPT 28 , DepthAnything 42, 43 , Marigold 17 and metric depth models that estimate depth with an absolute scale e.g., ZoeDepth 2 , Metric3D 45 , UniDepth 27 . Metric depth models require training with metric depth data that includes camera parameters 45 , thus their available training data is more limited compared to affine-invariant depth models, resulting in poorer generalization. Recently, Depth Anything V2 43 leveraged the Dinov2 23 pre-trained backbone to train an affine-invariant large-scale model using synthetic data with high-detail fidelity. This large model was then used as a teacher to distill smaller models on 62 million un- labeled datasets 42 , achieving SOTA performance in both generalization and geometric details. However, since Depth Anything V2 43 was trained exclusively on static images, thus lacks temporal consistency.', 'Related Work Consistent Video Depth Estimation': 'Consistent video depth estimation. The core task of consistent video depth estimation is to obtain temporal con- sistent and accuracy depth maps. Early methods for video depth relied on test-time training 18, 21, 48 , which were impractical for applications for their low efficiency. In re- cent years, learning-based models have emerged. Some of these models, such as MAMo 44 , use optical flow 33 to warp features, while others 29 depends on relative poses between frames to construct cost volumes. However, their performance were suffered from errors of inaccurate opti- cal flow or pose estimation. Additional approaches have attempted to enhance off-the-shelf monocular depth estima- tion MDE models with temporal stability model blocks 39 . Nevertheless, these efforts have not achieved satisfac- tory results due to suboptimal model designs and inadequate geometric consistency constraints. Furthermore, video diffu- sion models such as ChronoDepth 32 , DepthCrafter 13 , and DepthAnyVideo 41 show better details and temporal consistency. But they suffered from slow inference speeds and require extensive video depth training data. Limited by the large memory, these models 13 were typically tested only within the maximum window length used during train- ing, leading to depth flickering between windows and poor temporal and spatial consistency in long videos.', 'Video Depth Anything Overview': '3. Video Depth Anything In this section, we introduce Video Depth Anything, a feed- forward video transformer model to efficiently estimate tem- porally consistent video depth. We adopt the affine-invariant depth, but share the same scale and shift across the entire video. The pipeline of our method is shown in Fig. 2. Our model is built upon Depth Anything V2 with an additional temporal module and video dataset training Sec. 3.1 . A novel loss to enfoce temporal consistency is proposed in Sec. 3.2. Finally, a strategy combined with overlapping frames and key frames is presented to efficiently support super-long video inference Sect. 3.3 .', 'Video Depth Anything Architecture Encoder': '3.1. Architecture Due to the lack of sufficient video depth data, we start with a pre-trained image depth estimation model, Depth Anything V2, and adopt a joint training strategy using both image and video data. Depth Anything V2 Encoder. Depth Anything V2 43 is the current state-of-the-art monocular depth estimation model, characterized by its high accuracy and generalization capabilities. We use its trained model as our encoder. To reduce training costs and preserve well-learned features, the encoder is frozen during training. Unlike monocular depth encoders that only accept image input, our training scenario requires the encoder to process simultaneously both video and image data. To extract fea- tures from video frames with an image encoder, we collapse the temporal dimension of a video clip into the batch dimen- sion. The input data are denoted as X R B N C H W , where B represents the batch size,N is the number of frames in the video clip, N 1 for the image as input, C, H, W are the number of channels, height, width of the frames, re- spectively. The encoder takes X as input to produce a series of intermediate feature maps Fi R B N H p W p Ci, p is the patch size of the encoder. Although the image en- coder extracts strong visual representations from individual frames, it neglects the temporal information interactions be- tween frames. Thus, the spatiotemporal head is introduced to model the temporal relationship among the frames.', 'Video Depth Anything Architecture Spatiotemporal Head': 'Spatiotemporal Head. The spatiotemporal head STH is built upon the DPT 28 head and with the only modifica- tion being the insertion of temporal layers to capture tempo- ral information. A temporal layer consists of a multi-head self-attention 34 model SA and a feed-forward network FFN . When inputting a feature Fi into the temporal layer, the temporal dimension N is isolated, and self-attention is executed solely along the temporal dimension to facilitate the interaction of temporal features. To capture temporal positional relationships among different frames, we utilize absolute positional embedding to encode temporal positional information from the video sequence. The spatiotemporal head uniformly samples 4 feature maps from Fi including the final features from the encoder, denoted as F4 as inputs, and predicts a depth map D RH W . As shown in Figure 2, the selected features Fi are fed into the Reassemble layer to produce a feature pyramid. Then, the features are gradually fused from low resolution to high resolution by the Fusion layer. The Reassemble and Fusion layer are proposed by DPT 28 . The final fused high-resolution feature maps are passed through the output layer to produce the depth map D. To reduce the additional computational load, we insert the temporal layer at a few positions with lower feature resolutions.', 'Video Depth Anything Temporal Gradient Matching Loss': '3.2. Temporal Gradient Matching loss In this section, we start with the Optical Flow Based Warping OPW loss, then explore new loss designs and ultimately propose a Temporal Gradient Matching Loss TGM that does not rely on optical flow, yet still ensures the temporal consistency of predictions between frames. OPW loss. To constrain temporal consistency, previous video models such as 19, 38, 39 assume that the depths at corresponding positions in adjacent frames, identified Figure 2. Overall pipeline and the spatio-temporal head. Left Our model is composed of a backbone encoder from Depth Anything V2 and a newly proposed spatio-temporal head. We jointly train our model on video data using ground-truth depth labels for supervision and on unlabeled images with pseudo labels generated by a teacher model. During training, only the head is learned. Right Our spatiotemporal head inserts several temporal layers into the DPT head, while preserving the original structure of DPT head 28 . through optical flow, are consistent, e.g., the Optical Flow based Warping OPW loss proposed in NVDS 39 . OPW loss is computed after obtaining corresponding points on the basis of optical flow and warpi'}\n",
            "\n",
            "==================================================\n",
            "FINAL ANSWER\n",
            "==================================================\n",
            "The objective of this paper is to develop a model called Video Depth Anything that can provide high-quality, consistent depth estimation for super-long videos (over several minutes).\n",
            "\n",
            "This objective is further elaborated by:\n",
            "1.  **Addressing Temporal Inconsistency:** The model aims to overcome the temporal inconsistency issues, such as flickering and motion blur, prevalent in existing monocular depth estimation models (like Depth Anything) that are primarily designed for static images. (Abstract, Introduction Background and Problem)\n",
            "2.  **Overcoming Current Limitations:** It seeks to surpass the limitations of current methods, which are often restricted to short videos (around 10 seconds) and involve a trade-off between quality and computational efficiency. (Abstract, Introduction Existing Approaches)\n",
            "3.  **Achieving Comprehensive Performance:** Video Depth Anything, based on Depth Anything V2, aims to achieve temporal stability for arbitrarily long videos without sacrificing generalization ability, richness in details, or computational efficiency, and without introducing geometric or video generation priors. (Abstract, Introduction Our Solution)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/Video Depth Anything.pdf\"\n",
        "user_cmd = \"Tell me the section-wise summary of the paper\"\n",
        "\n",
        "state: ResearchState = {\n",
        "    \"file_path\": pdf_path,\n",
        "    \"user_prompt\": user_cmd,\n",
        "    \"pdf_pages\": [],\n",
        "    \"split_sections\": {},\n",
        "    \"figures\": \"\",\n",
        "    \"plan\": \"\",\n",
        "    \"final_answer\": \"\"\n",
        "}\n",
        "\n",
        "try:\n",
        "    print(\"\\n\\n===== PIPELINE START =====\")\n",
        "    result = graph.invoke(state)\n",
        "    print(\"===== PIPELINE END =====\\n\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SECTION SPLITTER OUTPUT\")\n",
        "    print(\"=\"*50)\n",
        "    print(result.get(\"split_sections\", {}))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"FINAL ANSWER\")\n",
        "    print(\"=\"*50)\n",
        "    print(result.get(\"final_answer\", \"\"))\n",
        "\n",
        "except Exception:\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z3HBs2LZFg_",
        "outputId": "6d169a3d-44be-49df-c4a5-8d1e5fe5a8e5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "===== PIPELINE START =====\n",
            "\n",
            "[AGENT] pdf_loader running...\n",
            "[pdf_loader] Loaded PDF successfully.\n",
            "\n",
            "[SCHEDULER] Deciding next agent...\n",
            "\n",
            "[AGENT] splitter running...\n",
            "[LLM] Splitter agent calling LLM...\n",
            "[splitter] CLEANED keys: ['Paper Metadata', 'Visual Overview Figure 1', 'Abstract Summary', 'Introduction to Monocular Depth Estimation Challenges', 'Existing Approaches and Limitations', 'Video Depth Anything Proposed Solution', 'Experimental Results and Performance Summary', 'Our Contributions', 'Related Work Monocular Depth Estimation', 'Related Work Consistent Video Depth Estimation', 'Video Depth Anything Overview', 'Architecture Depth Anything V2 Encoder', 'Architecture Spatiotemporal Head STH', 'Temporal Gradient Matching Loss Introduction']\n",
            "\n",
            "[AGENT] planner running...\n",
            "[LLM] Planner agent calling LLM...\n",
            "[planner] LLM call completed.\n",
            "\n",
            "[AGENT] final_answer running...\n",
            "[LLM] Final answer agent calling LLM...\n",
            "[final_answer] LLM call completed.\n",
            "===== PIPELINE END =====\n",
            "\n",
            "\n",
            "==================================================\n",
            "SECTION SPLITTER OUTPUT\n",
            "==================================================\n",
            "{'Paper Metadata': 'arXiv 2501.12375v3 cs.CV 15 Jun 2025 Video Depth Anything Consistent Depth Estimation for Super-Long Videos Sili Chen Hengkai Guo Shengnan Zhu Feihu Zhang Zilong Huang Jiashi Feng Bingyi Kang ByteDance videodepthanything.github.io', 'Visual Overview Figure 1': 'Figure 1. Left Our model can generate consistent depth predictions for long videos with rich actions. The demo video shows a 196-second 4690 frames long take of pair skating, as sourced from 14 . Right Comparison to baselines in terms of accuracy d1 , consistency, and latency on the Nvidia A100 GPU denoted with circle size . Consistency is defined as the maximum Temporal Alignment Error TAE among all models minus the TAE of each individual model. Our model achieves the best performance in all aspects.', 'Abstract Summary': 'Depth Anything has achieved remarkable success in monocular depth estimation with strong generalization ability. However, it suffers from temporal inconsistency in videos, hindering its practical applications. Various methods have been proposed to alleviate this issue by leveraging video generation models or introducing priors from optical flow and camera poses. Nonetheless, these methods are only applicable to short videos 10 seconds and require a trade-off between quality and computational efficiency. We propose Video Depth Anything for high-quality, consistent depth estimation in super-long videos over several minutes without sacrificing efficiency. We base our model on Depth Anything V2 and replace its head with an efficient spatial-temporal head. We design a straightforward yet effective temporal consistency loss by constraining the temporal depth gradient, eliminating the need for additional geometric priors. The model is trained on a joint dataset of video depth and unlabeled images, similar to Depth Anything V2. Moreover, a novel key-frame-based strategy is developed for long video inference. Experiments show that our model can be applied to arbitrarily long videos without compromising quality, consistency, or generalization ability. Comprehensive evaluations on multiple video benchmarks demonstrate that our approach sets a new state-of-the-art in zero-shot video depth estimation. We offer models of different scales to support a range of scenarios, with our smallest model capable of real-time performance at 30 FPS.', 'Introduction to Monocular Depth Estimation Challenges': 'Recently, monocular depth estimation MDE has made significant progress, as evidenced by advances in depth foundation models 3, 17, 42, 43 . For example, Depth Anything V2 43 demonstrates a strong generalization ability in producing depth predictions with rich details in various scenarios while being computationally efficient. However, these models have a major limitation they are mainly designed for static images and suffer from flickering and motion blur in videos. This limitation restricts their applications in robotics 8 , augmented reality 12 , and advanced video editing 25, 47 , which requires temporally consistent depth.', 'Existing Approaches and Limitations': 'Extensive efforts are being made to address this problem. Early work 18, 21, 48 often relies on test-time optimization to tune a pretrained monocular depth model with sophisticated geometry constraints. Given the heavy overhead at inference time of these methods, recent work mainly focuses on feedforward models and can be categorized into two approaches. The first approach 39 involves designing a plug-and-play module to augment monocular depth model predictions with temporal consistency. The training of such a module is highly dependent on optical flow 40 or camera poses 30 for consistency constraints, making the module susceptible to corresponding errors. The second approach 13, 32, 41 repurposes pre-trained video diffusion models 4 into video-to-depth models. These methods excel at producing fine-grained details, but are computationally inefficient, cannot leverage existing depth foundation models, and can only handle videos with limited length.', 'Video Depth Anything Proposed Solution': 'Then, a natural question arises Is it possible to have a model that can perfectly inherit the capabilities of existing foundation models while achieving temporal stability for arbitrarily long videos In this paper, we show that the answer is YES by developing Video Depth Anythingbased on Depth Anything V2, without sacrificing its generalization ability, richness in details, or computational efficiency. This target is achieved without introducing any geometric priors or video generation priors. Specifically, we first design a lightweight spatial-temporal head STH to replace the DPT head 28 and enable temporal information interactions. STH includes four temporal attention layers, applied along the temporal dimension for each spatial position. Introducing temporal attention only in the head prevents the learned representation from being corrupted by the limited video data. Then, we propose a temporal gradient matching loss to constrain depth prediction gradients along the temporal dimension to match those calculated from the ground truth. This loss function is jointly optimized with the scale-shift-invariant loss and spatial gradient matching loss 3, 42 . Despite its simplicity, it can effectively boost the model s temporal consistency. Third, to maintain the original capabilities of the model, we train it jointly on 730K video frames with depth annotations using supervised learning, and on 0.62 million unlabeled images using self-training, similar to Depth Anything V2 43 . To handle super-long videos at inference time, we developed a novel segment-wise processing strategy. Each new segment is concatenated with eight overlapping frames and two key frames from the previous video clips, forming a total of 32 frames. Then, the overlapping frames will be progressively interpolated between the two consecutive windows to ensure smoothness.', 'Experimental Results and Performance Summary': 'We compare our model with baselines on five datasets for zero-shot video depth estimation. Our model achieves state-of-the-art SOTA results on four of the datasets in terms of spatial accuracy and outperforms all baselines on all datasets in terms of temporal consistency. Not only can our model produce depth outputs visually comparable to video-diffusion-based methods, but it is also significantly more computationally efficient. For the first time, we can estimate consistent depth for videos over several minutes see Fig. 1 . Additionally, we tested our model for zero-shot image depth estimation on five datasets, noting only a marginal performance drop on one dataset compared to Depth Anything V2. As shown in Fig. 1 right , our model achieves the best performance in all three aspects spatial accuracy, temporal consistency, and computational efficiency.', 'Our Contributions': 'Our contributions are summarized as follows We develop a novel method to transform Depth Anything into Video Depth Anythingfor depth estimation in arbitrarily long videos. We propose a simple yet effective loss function that enforces temporal consistency constraints without introducing geometric or generative priors. Our model not only sets new SOTA both spatially and temporally in video depth estimation but is also the most computationally efficient.', 'Related Work Monocular Depth Estimation': 'Monocular depth estimation.Early monocular depth estimation 1, 9, 10, 46 efforts were primarily trained and tested on in-domain datasets. These models were constrained by the limited diversity of their training data, showing bad generalization for zero-shot application. Subsequently, MiDaS 3 introduced multi-dataset mixed training using an affine-invariant alignment method, significantly enhancing model generalization. However, due to limitations in the backbone model s performance and noise in the labeled depth data, the resulting depth maps lacked fine details. Following MiDaS 3 , monocular depth estimation models have generally bifurcated into two categories relative depth models that estimate affine-invariant depth e.g., DPT 28 , DepthAnything 42, 43 , Marigold 17 and metric depth models that estimate depth with an absolute scale e.g., ZoeDepth 2 , Metric3D 45 , UniDepth 27 . Metric depth models require training with metric depth data that includes camera parameters 45 , thus their available training data is more limited compared to affine-invariant depth models, resulting in poorer generalization. Recently, Depth Anything V2 43 leveraged the Dinov2 23 pre-trained backbone to train an affine-invariant large-scale model using synthetic data with high-detail fidelity. This large model was then used as a teacher to distill smaller models on 62 million unlabeled datasets 42 , achieving SOTA performance in both generalization and geometric details. However, since Depth Anything V2 43 was trained exclusively on static images, thus lacks temporal consistency.', 'Related Work Consistent Video Depth Estimation': 'Consistent video depth estimation.The core task of consistent video depth estimation is to obtain temporal consistent and accuracy depth maps. Early methods for video depth relied on test-time training 18, 21, 48 , which were impractical for applications for their low efficiency. In recent years, learning-based models have emerged. Some of these models, such as MAMo 44 , use optical flow 33 to warp features, while others 29 depends on relative poses between frames to construct cost volumes. However, their performance were suffered from errors of inaccurate optical flow or pose estimation. Additional approaches have attempted to enhance off-the-shelf monocular depth estimation MDE models with temporal stability model blocks 39 . Nevertheless, these efforts have not achieved satisfactory results due to suboptimal model designs and inadequate geometric consistency constraints. Furthermore, video diffusion models such as ChronoDepth 32 , DepthCrafter 13 , and DepthAnyVideo 41 show better details and temporal consistency. But they suffered from slow inference speeds and require extensive video depth training data. Limited by the large memory, these models 13 were typically tested only within the maximum window length used during training, leading to depth flickering between windows and poor temporal and spatial consistency in long videos.', 'Video Depth Anything Overview': 'In this section, we introduce Video Depth Anything, a feed-forward video transformer model to efficiently estimate temporally consistent video depth. We adopt the affine-invariant depth, but share the same scale and shift across the entire video. The pipeline of our method is shown in Fig. 2. Our model is built upon Depth Anything V2 with an additional temporal module and video dataset training Sec. 3.1 . A novel loss to enfoce temporal consistency is proposed in Sec. 3.2. Finally, a strategy combined with overlapping frames and key frames is presented to efficiently support super-long video inference Sect. 3.3 .', 'Architecture Depth Anything V2 Encoder': 'Due to the lack of sufficient video depth data, we start with a pre-trained image depth estimation model, Depth Anything V2, and adopt a joint training strategy using both image and video data. Depth Anything V2 Encoder.Depth Anything V2 43 is the current state-of-the-art monocular depth estimation model, characterized by its high accuracy and generalization capabilities. We use its trained model as our encoder. To reduce training costs and preserve well-learned features, the encoder is frozen during training. Unlike monocular depth encoders that only accept image input, our training scenario requires the encoder to process simultaneously both video and image data. To extract features from video frames with an image encoder, we collapse the temporal dimension of a video clip into the batch dimension. The input data are denoted as X with dimensions BxN xCxHxW , where B represents the batch size, N is the number of frames in the video clip, N 1 for the image as input, C, H, W are the number of channels, height, width of the frames, respectively. The encoder takes X as input to produce a series of intermediate feature maps Fi with dimensions BxN x H p x W p xCi, p is the patch size of the encoder. Although the image encoder extracts strong visual representations from individual frames, it neglects the temporal information interactions between frames. Thus, the spatiotemporal head is introduced to model the temporal relationship among the frames.', 'Architecture Spatiotemporal Head STH': 'Spatiotemporal Head.The spatiotemporal head STH is built upon the DPT 28 head and with the only modification being the insertion of temporal layers to capture temporal information. A temporal layer consists of a multi-head self-attention 34 model SA and a feed-forward network FFN . When inputting a feature Fi into the temporal layer, the temporal dimension N is isolated, and self-attention is executed solely along the temporal dimension to facilitate the interaction of temporal features. To capture temporal positional relationships among different frames, we utilize absolute positional embedding to encode temporal positional information from the video sequence. The spatiotemporal head uniformly samples 4 feature maps from Fi including the final features from the encoder, denoted as F4 as inputs, and predicts a depth map D with dimensions HxW . As shown in Figure 2, the selected features Fi are fed into the Reassemble layer to produce a feature pyramid. Then, the features are gradually fused from low resolution to high resolution by the Fusion layer. The Reassemble and Fusion layer are proposed by DPT 28 . The final fused high-resolution feature maps are passed through the output layer to produce the depth map D. To reduce the additional computational load, we insert the temporal layer at a few positions with lower feature resolutions.', 'Temporal Gradient Matching Loss Introduction': '3.2. Temporal Gradient Matching loss In this section, we start with the Optical Flow Based Warping OPW loss, then explore new loss designs and ultimately propose a Temporal Gradient Matching Loss TGM that does not rely on optical flow, yet still ensures the temporal consistency of predictions between frames. OPW loss. To constrain temporal consistency, previous video models such as 19, 38, 39 assume that the depths at corresponding positions in adjacent frames, identified through optical flow, are consistent, e.g., the Optical Flow based Warping OPW loss proposed in NVDS 39 . OPW loss is computed after obtaining corresponding points on the basis of optical flow and warpi'}\n",
            "\n",
            "==================================================\n",
            "FINAL ANSWER\n",
            "==================================================\n",
            "Here is the section-wise summary of the paper:\n",
            "\n",
            "1.  **Paper Metadata**: This paper, titled \"Video Depth Anything: Consistent Depth Estimation for Super-Long Videos\" (arXiv 2501.12375v3), is authored by Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, and Bingyi Kang from ByteDance.\n",
            "2.  **Visual Overview Figure 1**: Figure 1 demonstrates the model's ability to generate consistent depth predictions for long videos (e.g., a 196-second pair skating video). It also visually compares the model's accuracy, consistency, and latency against baselines, showing superior performance across all metrics.\n",
            "3.  **Abstract Summary**: While Depth Anything excels in monocular depth estimation, it lacks temporal consistency in videos. Existing methods for video depth are limited to short videos or involve trade-offs. This paper proposes Video Depth Anything, based on Depth Anything V2, which replaces its head with an efficient spatial-temporal head to achieve high-quality, consistent depth estimation for super-long videos (several minutes) without sacrificing efficiency.\n",
            "4.  **Introduction to Monocular Depth Estimation Challenges**: Monocular Depth Estimation (MDE) has advanced significantly with models like Depth Anything V2 offering strong generalization. However, these models are designed for static images and suffer from flickering and motion blur in videos, which restricts their application in areas requiring temporal consistency like robotics and AR.\n",
            "5.  **Existing Approaches and Limitations**: Efforts to address temporal inconsistency include test-time optimization (heavy overhead) and feedforward models. The latter category involves either plug-and-play modules (reliant on optical flow or camera poses, susceptible to errors) or repurposing pre-trained video diffusion models (excel at quality but limited to short videos, often with efficiency trade-offs).\n",
            "6.  **Video Depth Anything Proposed Solution**: The paper introduces Video Depth Anything, built upon Depth Anything V2, to achieve temporal stability for arbitrarily long videos while retaining generalization, detail, and efficiency. It avoids geometric or video generation priors. The core innovation is a lightweight spatial-temporal head (STH) that replaces the DPT head, incorporating four temporal attention layers for temporal information interaction.\n",
            "7.  **Experimental Results and Performance Summary**: The model achieves state-of-the-art (SOTA) results on four out of five datasets for spatial accuracy and outperforms all baselines in temporal consistency across all datasets in zero-shot video depth estimation. It is also significantly more computationally efficient than video-diffusion-based methods and can estimate consistent depth for videos over several minutes. For image depth estimation, it shows only a marginal performance drop compared to Depth Anything V2.\n",
            "8.  **Our Contributions**: The key contributions include developing Video Depth Anything for arbitrarily long videos, proposing a simple yet effective loss function for temporal consistency without geometric or generative priors, and setting new SOTA in both spatial and temporal aspects of video depth estimation while being highly computationally efficient.\n",
            "9.  **Related Work Monocular Depth Estimation**: This section traces the evolution of MDE from early in-domain models with poor generalization to MiDaS, which introduced multi-dataset training and improved generalization. It notes the subsequent divergence into relative (e.g., DPT, DepthAnything) and metric depth models.\n",
            "10. **Related Work Consistent Video Depth Estimation**: Early methods relied on inefficient test-time training. More recent learning-based models use optical flow or relative poses, but their performance suffered from estimation errors. Other approaches tried to add temporal stability blocks to off-the-shelf MDE models, but with limited success.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/Video Depth Anything.pdf\"\n",
        "user_cmd = \"Summarize the architecture in 2 lines\"\n",
        "\n",
        "state: ResearchState = {\n",
        "    \"file_path\": pdf_path,\n",
        "    \"user_prompt\": user_cmd,\n",
        "    \"pdf_pages\": [],\n",
        "    \"split_sections\": {},\n",
        "    \"figures\": \"\",\n",
        "    \"plan\": \"\",\n",
        "    \"final_answer\": \"\"\n",
        "}\n",
        "\n",
        "try:\n",
        "    print(\"\\n\\n===== PIPELINE START =====\")\n",
        "    result = graph.invoke(state)\n",
        "    print(\"===== PIPELINE END =====\\n\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SECTION SPLITTER OUTPUT\")\n",
        "    print(\"=\"*50)\n",
        "    print(result.get(\"split_sections\", {}))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"FINAL ANSWER\")\n",
        "    print(\"=\"*50)\n",
        "    print(result.get(\"final_answer\", \"\"))\n",
        "\n",
        "except Exception:\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TnGyxo9panU",
        "outputId": "31180aa6-b090-42df-a133-970657d853a1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "===== PIPELINE START =====\n",
            "\n",
            "[AGENT] pdf_loader running...\n",
            "[pdf_loader] Loaded PDF successfully.\n",
            "\n",
            "[SCHEDULER] Deciding next agent...\n",
            "\n",
            "[AGENT] splitter running...\n",
            "[LLM] Splitter agent calling LLM...\n",
            "[splitter] CLEANED keys: ['Paper Identification', 'Figure 1 Visual Summary', 'Abstract', 'Introduction Problem Statement', 'Introduction Existing Solutions', 'Our Approach Overview', 'Key Technical Components of Our Approach', 'Introduction Experimental Results and Contributions', 'Related Work Monocular Depth Estimation', 'Related Work Consistent Video Depth Estimation', 'Video Depth Anything Overview', 'Architecture Depth Anything V2 Encoder', 'Architecture Spatiotemporal Head', 'Temporal Gradient Matching Loss Introduction']\n",
            "\n",
            "[AGENT] planner running...\n",
            "[LLM] Planner agent calling LLM...\n",
            "[planner] LLM call completed.\n",
            "\n",
            "[AGENT] final_answer running...\n",
            "[LLM] Final answer agent calling LLM...\n",
            "[final_answer] LLM call completed.\n",
            "===== PIPELINE END =====\n",
            "\n",
            "\n",
            "==================================================\n",
            "SECTION SPLITTER OUTPUT\n",
            "==================================================\n",
            "{'Paper Identification': 'arXiv 2501.12375v3 cs.CV 15 Jun 2025 Video Depth Anything Consistent Depth Estimation for Super-Long Videos Sili Chen Hengkai Guo Shengnan Zhu Feihu Zhang Zilong Huang Jiashi Feng Bingyi Kang ByteDance videodepthanything.github.io Corresponding author.', 'Figure 1 Visual Summary': 'Figure 1. Left Our model can generate consistent depth predictions for long videos with rich actions. The demo video shows a 196-second 4690 frames long take of pair skating, as sourced from 14 . Right Comparison to baselines in terms of accuracy 1 , consistency, and latency on the Nvidia A100 GPU denoted with circle size . Consistency is defined as the maximum Temporal Alignment Error TAE among all models minus the TAE of each individual model. Our model achieves the best performance in all aspects.', 'Abstract': 'Depth Anything has achieved remarkable success in monocular depth estimation with strong generalization ability. However, it suffers from temporal inconsistency in videos, hindering its practical applications. Various methods have been proposed to alleviate this issue by leveraging video generation models or introducing priors from optical flow and camera poses. Nonetheless, these methods are only applicable to short videos 10 seconds and require a trade-off between quality and computational efficiency. We propose Video Depth Anything for high-quality, consistent depth estimation in super-long videos over several minutes without sacrificing efficiency. We base our model on Depth Anything V2 and replace its head with an efficient spatial-temporal head. We design a straightforward yet effective temporal consistency loss by constraining the temporal depth gradient, eliminating the need for additional geometric priors. The model is trained on a joint dataset of video depth and unlabeled images, similar to Depth Anything V2. Moreover, a novel key-frame-based strategy is developed for long video inference. Experiments show that our model can be applied to arbitrarily long videos without compromising quality, consistency, or generalization ability. Comprehensive evaluations on multiple video benchmarks demonstrate that our approach sets a new state-of-the-art in zero-shot video depth estimation. We offer models of different scales to support a range of scenarios, with our smallest model capable of real-time performance at 30 FPS.', 'Introduction Problem Statement': '1. Introduction Recently, monocular depth estimation MDE has made significant progress, as evidenced by advances in depth foundation models 3, 17, 42, 43 . For example, Depth Anything V2 43 demonstrates a strong generalization ability in producing depth predictions with rich details in various scenarios while being computationally efficient. However, these models have a major limitation they are mainly designed for static images and suffer from flickering and motion blur in videos. This limitation restricts their applications in robotics 8 , augmented reality 12 , and advanced video editing 25, 47 , which requires temporally consistent depth.', 'Introduction Existing Solutions': 'Extensive efforts are being made to address this problem. Early work 18, 21, 48 often relies on test-time optimization to tune a pretrained monocular depth model with sophisticated geometry constraints. Given the heavy overhead at inference time of these methods, recent work mainly focuses on feedforward models and can be categorized into two approaches. The first approach 39 involves designing a plug-and-play module to augment monocular depth model predictions with temporal consistency. The training of such a module is highly dependent on optical flow 40 or camera poses 30 for consistency constraints, making the module susceptible to corresponding errors. The second approach 13, 32, 41 repurposes pre-trained video diffusion models 4 into video-to-depth models. These methods excel at producing fine-grained details, but are computationally inefficient, cannot leverage existing depth foundation models, and can only handle videos with limited length.', 'Our Approach Overview': 'Then, a natural question arises Is it possible to have a model that can perfectly inherit the capabilities of existing foundation models while achieving temporal stability for arbitrarily long videos In this paper, we show that the answer is YES by developing Video Depth Anythingbased on Depth Anything V2, without sacrificing its generalization ability, richness in details, or computational efficiency. This target is achieved without introducing any geometric priors or video generation priors.', 'Key Technical Components of Our Approach': 'Specifically, we first design a lightweight spatial-temporal head STH to replace the DPT head 28 and enable temporal information interactions. STH includes four temporal attention layers, applied along the temporal dimension for each spatial position. Introducing temporal attention only in the head prevents the learned representation from being corrupted by the limited video data. Then, we propose a temporal gradient matching loss to constrain depth prediction gradients along the temporal dimension to match those calculated from the ground truth. This loss function is jointly optimized with the scale-shift-invariant loss and spatial gradient matching loss 3, 42 . Despite its simplicity, it can effectively boost the model s temporal consistency. Third, to maintain the original capabilities of the model, we train it jointly on 730K video frames with depth annotations using supervised learning, and on 0.62 million unlabeled images using self-training, similar to Depth Anything V2 43 . To handle super-long videos at inference time, we developed a novel segment-wise processing strategy. Each new segment is concatenated with eight overlapping frames and two key frames from the previous video clips, forming a total of 32 frames. Then, the overlapping frames will be progressively interpolated between the two consecutive windows to ensure smoothness.', 'Introduction Experimental Results and Contributions': 'We compare our model with baselines on five datasets for zero-shot video depth estimation. Our model achieves state-of-the-art SOTA results on four of the datasets in terms of spatial accuracy and outperforms all baselines on all datasets in terms of temporal consistency. Not only can our model produce depth outputs visually comparable to video-diffusion-based methods, but it is also significantly more computationally efficient. For the first time, we can estimate consistent depth for videos over several minutes see Fig. 1 . Additionally, we tested our model for zero-shot image depth estimation on five datasets, noting only a marginal performance drop on one dataset compared to Depth Anything V2. As shown in Fig. 1 right , our model achieves the best performance in all three aspects spatial accuracy, temporal consistency, and computational efficiency. Our contributions are summarized as follows We develop a novel method to transform Depth Anything into Video Depth Anythingfor depth estimation in arbitrarily long videos. We propose a simple yet effective loss function that enforces temporal consistency constraints without introducing geometric or generative priors. Our model not only sets new SOTA both spatially and temporally in video depth estimation but is also the most computationally efficient.', 'Related Work Monocular Depth Estimation': '2. Related Work Monocular depth estimation.Early monocular depth estimation 1, 9, 10, 46 efforts were primarily trained and tested on in-domain datasets. These models were constrained by the limited diversity of their training data, showing bad generalization for zero-shot application. Subsequently, MiDaS 3 introduced multi-dataset mixed training using an affine-invariant alignment method, significantly enhancing model generalization. However, due to limitations in the backbone model s performance and noise in the labeled depth data, the resulting depth maps lacked fine details. Following MiDaS 3 , monocular depth estimation models have generally bifurcated into two categories relative depth models that estimate affine-invariant depth e.g., DPT 28 , DepthAnything 42, 43 , Marigold 17 and metric depth models that estimate depth with an absolute scale e.g., ZoeDepth 2 , Metric3D 45 , UniDepth 27 . Metric depth models require training with metric depth data that includes camera parameters 45 , thus their available training data is more limited compared to affine-invariant depth models, resulting in poorer generalization. Recently, Depth Anything V2 43 leveraged the Dinov2 23 pre-trained backbone to train an affine-invariant large-scale model using synthetic data with high-detail fidelity. This large model was then used as a teacher to distill smaller models on 62 million unlabeled datasets 42 , achieving SOTA performance in both generalization and geometric details. However, since Depth Anything V2 43 was trained exclusively on static images, thus lacks temporal consistency.', 'Related Work Consistent Video Depth Estimation': 'Consistent video depth estimation.The core task of consistent video depth estimation is to obtain temporal consistent and accuracy depth maps. Early methods for video depth relied on test-time training 18, 21, 48 , which were impractical for applications for their low efficiency. In recent years, learning-based models have emerged. Some of these models, such as MAMo 44 , use optical flow 33 to warp features, while others 29 depends on relative poses between frames to construct cost volumes. However, their performance were suffered from errors of inaccurate optical flow or pose estimation. Additional approaches have attempted to enhance off-the-shelf monocular depth estimation MDE models with temporal stability model blocks 39 . Nevertheless, these efforts have not achieved satisfactory results due to suboptimal model designs and inadequate geometric consistency constraints. Furthermore, video diffusion models such as ChronoDepth 32 , DepthCrafter 13 , and DepthAnyVideo 41 show better details and temporal consistency. But they suffered from slow inference speeds and require extensive video depth training data. Limited by the large memory, these models 13 were typically tested only within the maximum window length used during training, leading to depth flickering between windows and poor temporal and spatial consistency in long videos.', 'Video Depth Anything Overview': '3. Video Depth Anything In this section, we introduce Video Depth Anything, a feed-forward video transformer model to efficiently estimate temporally consistent video depth. We adopt the affine-invariant depth, but share the same scale and shift across the entire video. The pipeline of our method is shown in Fig. 2. Our model is built upon Depth Anything V2 with an additional temporal module and video dataset training Sec. 3.1 . A novel loss to enfoce temporal consistency is proposed in Sec. 3.2. Finally, a strategy combined with overlapping frames and key frames is presented to efficiently support super-long video inference Sect. 3.3 .', 'Architecture Depth Anything V2 Encoder': '3.1. Architecture Due to the lack of sufficient video depth data, we start with a pre-trained image depth estimation model, Depth Anything V2, and adopt a joint training strategy using both image and video data. Depth Anything V2 Encoder.Depth Anything V2 43 is the current state-of-the-art monocular depth estimation model, characterized by its high accuracy and generalization capabilities. We use its trained model as our encoder. To reduce training costs and preserve well-learned features, the encoder is frozen during training. Unlike monocular depth encoders that only accept image input, our training scenario requires the encoder to process simultaneously both video and image data. To extract features from video frames with an image encoder, we collapse the temporal dimension of a video clip into the batch dimension. The input data are denoted as X R B N C H W , where B represents the batch size,N is the number of frames in the video clip, N 1 for the image as input, C, H, W are the number of channels, height, width of the frames, respectively. The encoder takes X as input to produce a series of intermediate feature maps Fi R B N H p W p Ci, p is the patch size of the encoder. Although the image encoder extracts strong visual representations from individual frames, it neglects the temporal information interactions between frames. Thus, the spatiotemporal head is introduced to model the temporal relationship among the frames.', 'Architecture Spatiotemporal Head': 'Spatiotemporal Head.The spatiotemporal head STH is built upon the DPT 28 head and with the only modification being the insertion of temporal layers to capture temporal information. A temporal layer consists of a multi-head self-attention 34 model SA and a feed-forward network FFN . When inputting a feature Fi into the temporal layer, the temporal dimension N is isolated, and self-attention is executed solely along the temporal dimension to facilitate the interaction of temporal features. To capture temporal positional relationships among different frames, we utilize absolute positional embedding to encode temporal positional information from the video sequence. The spatiotemporal head uniformly samples 4 feature maps from Fi including the final features from the encoder, denoted as F4 as inputs, and predicts a depth map D RH W . As shown in Figure 2, the selected features Fi are fed into the Reassemble layer to produce a feature pyramid. Then, the features are gradually fused from low resolution to high resolution by the Fusion layer. The Reassemble and Fusion layer are proposed by DPT 28 . The final fused high-resolution feature maps are passed through the output layer to produce the depth map D. To reduce the additional computational load, we insert the temporal layer at a few positions with lower feature resolutions.', 'Temporal Gradient Matching Loss Introduction': '3.2. Temporal Gradient Matching loss In this section, we start with the Optical Flow Based Warping OPW loss, then explore new loss designs and ultimately propose a Temporal Gradient Matching Loss TGM that does not rely on optical flow, yet still ensures the temporal consistency of predictions between frames. OPW loss. To constrain temporal consistency, previous video models such as 19, 38, 39 assume that the depths at corresponding positions in adjacent frames, identified through optical flow, are consistent, e.g., the Optical Flow based Warping OPW loss proposed in NVDS 39 . OPW loss is computed after obtaining corresponding points on the basis of optical flow and warpi'}\n",
            "\n",
            "==================================================\n",
            "FINAL ANSWER\n",
            "==================================================\n",
            "1. Video Depth Anything is built upon the frozen Depth Anything V2 encoder, which processes individual frames by collapsing the temporal dimension into the batch dimension.\n",
            "2. Its key architectural modification is replacing the original DPT head with a lightweight Spatial-Temporal Head (STH) that incorporates temporal attention layers to facilitate inter-frame information interactions, optimized with a temporal gradient matching loss for consistency.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/Video Depth Anything.pdf\"\n",
        "user_cmd = \"What figures and tables are present in the paper\"\n",
        "\n",
        "state: ResearchState = {\n",
        "    \"file_path\": pdf_path,\n",
        "    \"user_prompt\": user_cmd,\n",
        "    \"pdf_pages\": [],\n",
        "    \"split_sections\": {},\n",
        "    \"figures\": \"\",\n",
        "    \"plan\": \"\",\n",
        "    \"final_answer\": \"\"\n",
        "}\n",
        "\n",
        "try:\n",
        "    print(\"\\n\\n===== PIPELINE START =====\")\n",
        "    result = graph.invoke(state)\n",
        "    print(\"===== PIPELINE END =====\\n\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SECTION SPLITTER OUTPUT\")\n",
        "    print(\"=\"*50)\n",
        "    print(result.get(\"split_sections\", {}))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"FINAL ANSWER\")\n",
        "    print(\"=\"*50)\n",
        "    print(result.get(\"final_answer\", \"\"))\n",
        "\n",
        "except Exception:\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N436ekEhrvia",
        "outputId": "abc9166c-ae32-4386-8ae9-569c2489a8e9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "===== PIPELINE START =====\n",
            "\n",
            "[AGENT] pdf_loader running...\n",
            "[pdf_loader] Loaded PDF successfully.\n",
            "\n",
            "[SCHEDULER] Deciding next agent...\n",
            "\n",
            "[AGENT] splitter running...\n",
            "[LLM] Splitter agent calling LLM...\n",
            "[splitter] CLEANED keys: ['arXiv Info and Figure 1', 'Abstract', 'Introduction Monocular Depth Challenges', 'Introduction Related Work and Problem', 'Introduction Video Depth Anything Solution', 'Introduction Experimental Results and Contributions', 'Related Work Monocular Depth Estimation', 'Related Work Consistent Video Depth Estimation', 'Video Depth Anything Overview', 'Architecture Joint Training and Encoder', 'Architecture Spatiotemporal Head', 'Temporal Gradient Matching Loss']\n",
            "\n",
            "[AGENT] planner running...\n",
            "[LLM] Planner agent calling LLM...\n",
            "[planner] LLM call completed.\n",
            "\n",
            "[AGENT] final_answer running...\n",
            "[LLM] Final answer agent calling LLM...\n",
            "[final_answer] LLM call completed.\n",
            "===== PIPELINE END =====\n",
            "\n",
            "\n",
            "==================================================\n",
            "SECTION SPLITTER OUTPUT\n",
            "==================================================\n",
            "{'arXiv Info and Figure 1': 'arXiv 2501.12375v3 cs.CV 15 Jun 2025 Video Depth Anything Consistent Depth Estimation for Super-Long Videos Sili Chen Hengkai Guo Shengnan Zhu Feihu Zhang Zilong Huang Jiashi Feng Bingyi Kang ByteDance videodepthanything.github.io Corresponding author. 3s 22s 42s 59s 79s 99s 122s 140s 161s 175s Figure 1. Left Our model can generate consistent depth predictions for long videos with rich actions. The demo video shows a 196-second 4690 frames long take of pair skating, as sourced from 14 . Right Comparison to baselines in terms of accuracy 1 , consistency, and latency on the Nvidia A100 GPU denoted with circle size . Consistency is defined as the maximum Temporal Alignment Error TAE among all models minus the TAE of each individual model. Our model achieves the best performance in all aspects.', 'Abstract': 'Depth Anything has achieved remarkable success in monocular depth estimation with strong generalization ability. However, it suffers from temporal inconsistency in videos, hindering its practical applications. Various methods have been proposed to alleviate this issue by leveraging video generation models or introducing priors from optical flow and camera poses. Nonetheless, these methods are only applicable to short videos 10 seconds and require a trade-off between quality and computational efficiency. We propose Video Depth Anything for high-quality, consistent depth estimation in super-long videos over several minutes without sacrificing efficiency. We base our model on Depth Anything V2 and replace its head with an efficient spatial-temporal head. We design a straightforward yet effective temporal consistency loss by constraining the temporal depth gradient, eliminating the need for additional geometric priors. The model is trained on a joint dataset of video depth and unlabeled images, similar to Depth Anything V2. Moreover, a novel key-frame-based strategy is developed for long video inference. Experiments show that our model can be applied to arbitrarily long videos without compromising quality, consistency, or generalization ability. Comprehensive evaluations on multiple video benchmarks demonstrate that our approach sets a new state-of-the-art in zero-shot video depth estimation. We offer models of different scales to support a range of scenarios, with our smallest model capable of real-time performance at 30 FPS.', 'Introduction Monocular Depth Challenges': 'Recently, monocular depth estimation MDE has made significant progress, as evidenced by advances in depth foundation models 3, 17, 42, 43 . For example, Depth Anything V2 43 demonstrates a strong generalization ability in producing depth predictions with rich details in various scenarios while being computationally efficient. However, these models have a major limitation they are mainly designed for static images and suffer from flickering and motion blur in videos. This limitation restricts their applications in robotics 8 , augmented reality 12 , and advanced video editing 25, 47 , which requires temporally consistent depth.', 'Introduction Related Work and Problem': 'Extensive efforts are being made to address this problem. Early work 18, 21, 48 often relies on test-time optimization to tune a pretrained monocular depth model with sophisticated geometry constraints. Given the heavy overhead at inference time of these methods, recent work mainly focuses on feedforward models and can be categorized into two approaches. The first approach 39 involves designing a plug-and-play module to augment monocular depth model predictions with temporal consistency. The training of such a module is highly dependent on optical flow 40 or camera poses 30 for consistency constraints, making the module susceptible to corresponding errors. The second approach 13, 32, 41 repurposes pre-trained video diffusion models 4 into video-to-depth models. These methods excel at producing fine-grained details, but are computationally inefficient, cannot leverage existing depth foundation models, and can only handle videos with limited length. Then, a natural question arises Is it possible to have a model that can perfectly inherit the capabilities of existing foundation models while achieving temporal stability for arbitrarily long videos', 'Introduction Video Depth Anything Solution': 'In this paper, we show that the answer is YES by developing Video Depth Anythingbased on Depth Anything V2, without sacrificing its generalization ability, richness in details, or computational efficiency. This target is achieved without introducing any geometric priors or video generation priors. Specifically, we first design a lightweight spatial-temporal head STH to replace the DPT head 28 and enable temporal information interactions. STH includes four temporal attention layers, applied along the temporal dimension for each spatial position. Introducing temporal attention only in the head prevents the learned representation from being corrupted by the limited video data. Then, we propose a temporal gradient matching loss to constrain depth prediction gradients along the temporal dimension to match those calculated from the ground truth. This loss function is jointly optimized with the scale-shift-invariant loss and spatial gradient matching loss 3, 42 . Despite its simplicity, it can effectively boost the model s temporal consistency. Third, to maintain the original capabilities of the model, we train it jointly on 730K video frames with depth annotations using supervised learning, and on 0.62 million unlabeled images using self-training, similar to Depth Anything V2 43 . To handle super-long videos at inference time, we developed a novel segment-wise processing strategy. Each new segment is concatenated with eight overlapping frames and two key frames from the previous video clips, forming a total of 32 frames. Then, the overlapping frames will be progressively interpolated between the two consecutive windows to ensure smoothness.', 'Introduction Experimental Results and Contributions': 'We compare our model with baselines on five datasets for zero-shot video depth estimation. Our model achieves state-of-the-art SOTA results on four of the datasets in terms of spatial accuracy and outperforms all baselines on all datasets in terms of temporal consistency. Not only can our model produce depth outputs visually comparable to video-diffusion-based methods, but it is also significantly more computationally efficient. For the first time, we can estimate consistent depth for videos over several minutes see Fig. 1 . Additionally, we tested our model for zero-shot image depth estimation on five datasets, noting only a marginal performance drop on one dataset compared to Depth Anything V2. As shown in Fig. 1 right , our model achieves the best performance in all three aspects spatial accuracy, temporal consistency, and computational efficiency. Our contributions are summarized as follows We develop a novel method to transform Depth Anything into Video Depth Anythingfor depth estimation in arbitrarily long videos. We propose a simple yet effective loss function that enforces temporal consistency constraints without introducing geometric or generative priors. Our model not only sets new SOTA both spatially and temporally in video depth estimation but is also the most computationally efficient.', 'Related Work Monocular Depth Estimation': 'Monocular depth estimation.Early monocular depth estimation 1, 9, 10, 46 efforts were primarily trained and tested on in-domain datasets. These models were constrained by the limited diversity of their training data, showing bad generalization for zero-shot application. Subsequently, MiDaS 3 introduced multi-dataset mixed training using an affine-invariant alignment method, significantly enhancing model generalization. However, due to limitations in the backbone model s performance and noise in the labeled depth data, the resulting depth maps lacked fine details. Following MiDaS 3 , monocular depth estimation models have generally bifurcated into two categories relative depth models that estimate affine-invariant depth e.g., DPT 28 , DepthAnything 42, 43 , Marigold 17 and metric depth models that estimate depth with an absolute scale e.g., ZoeDepth 2 , Metric3D 45 , UniDepth 27 . Metric depth models require training with metric depth data that includes camera parameters 45 , thus their available training data is more limited compared to affine-invariant depth models, resulting in poorer generalization. Recently, Depth Anything V2 43 leveraged the Dinov2 23 pre-trained backbone to train an affine-invariant large-scale model using synthetic data with high-detail fidelity. This large model was then used as a teacher to distill smaller models on 62 million unlabeled datasets 42 , achieving SOTA performance in both generalization and geometric details. However, since Depth Anything V2 43 was trained exclusively on static images, thus lacks temporal consistency.', 'Related Work Consistent Video Depth Estimation': 'Consistent video depth estimation.The core task of consistent video depth estimation is to obtain temporal consistent and accuracy depth maps. Early methods for video depth relied on test-time training 18, 21, 48 , which were impractical for applications for their low efficiency. In recent years, learning-based models have emerged. Some of these models, such as MAMo 44 , use optical flow 33 to warp features, while others 29 depends on relative poses between frames to construct cost volumes. However, their performance were suffered from errors of inaccurate optical flow or pose estimation. Additional approaches have attempted to enhance off-the-shelf monocular depth estimation MDE models with temporal stability model blocks 39 . Nevertheless, these efforts have not achieved satisfactory results due to suboptimal model designs and inadequate geometric consistency constraints. Furthermore, video diffusion models such as ChronoDepth 32 , DepthCrafter 13 , and DepthAnyVideo 41 show better details and temporal consistency. But they suffered from slow inference speeds and require extensive video depth training data. Limited by the large memory, these models 13 were typically tested only within the maximum window length used during training, leading to depth flickering between windows and poor temporal and spatial consistency in long videos.', 'Video Depth Anything Overview': 'In this section, we introduce Video Depth Anything, a feed-forward video transformer model to efficiently estimate temporally consistent video depth. We adopt the affine-invariant depth, but share the same scale and shift across the entire video. The pipeline of our method is shown in Fig. 2. Our model is built upon Depth Anything V2 with an additional temporal module and video dataset training Sec. 3.1 . A novel loss to enfoce temporal consistency is proposed in Sec. 3.2. Finally, a strategy combined with overlapping frames and key frames is presented to efficiently support super-long video inference Sect. 3.3 .', 'Architecture Joint Training and Encoder': 'Due to the lack of sufficient video depth data, we start with a pre-trained image depth estimation model, Depth Anything V2, and adopt a joint training strategy using both image and video data. Depth Anything V2 Encoder.Depth Anything V2 43 is the current state-of-the-art monocular depth estimation model, characterized by its high accuracy and generalization capabilities. We use its trained model as our encoder. To reduce training costs and preserve well-learned features, the encoder is frozen during training. Unlike monocular depth encoders that only accept image input, our training scenario requires the encoder to process simultaneously both video and image data. To extract features from video frames with an image encoder, we collapse the temporal dimension of a video clip into the batch dimension. The input data are denoted as X R B N C H W , where B represents the batch size,N is the number of frames in the video clip, N 1 for the image as input, C, H, W are the number of channels, height, width of the frames, respectively. The encoder takes X as input to produce a series of intermediate feature maps Fi R B N H p W p Ci, p is the patch size of the encoder. Although the image encoder extracts strong visual representations from individual frames, it neglects the temporal information interactions between frames. Thus, the spatiotemporal head is introduced to model the temporal relationship among the frames.', 'Architecture Spatiotemporal Head': 'Spatiotemporal Head.The spatiotemporal head STH is built upon the DPT 28 head and with the only modification being the insertion of temporal layers to capture temporal information. A temporal layer consists of a multi-head self-attention 34 model SA and a feed-forward network FFN . When inputting a feature Fi into the temporal layer, the temporal dimension N is isolated, and self-attention is executed solely along the temporal dimension to facilitate the interaction of temporal features. To capture temporal positional relationships among different frames, we utilize absolute positional embedding to encode temporal positional information from the video sequence. The spatiotemporal head uniformly samples 4 feature maps from Fi including the final features from the encoder, denoted as F4 as inputs, and predicts a depth map D RH W . As shown in Figure 2, the selected features Fi are fed into the Reassemble layer to produce a feature pyramid. Then, the features are gradually fused from low resolution to high resolution by the Fusion layer. The Reassemble and Fusion layer are proposed by DPT 28 . The final fused high-resolution feature maps are passed through the output layer to produce the depth map D. To reduce the additional computational load, we insert the temporal layer at a few positions with lower feature resolutions.', 'Temporal Gradient Matching Loss': 'In this section, we start with the Optical Flow Based Warping OPW loss, then explore new loss designs and ultimately propose a Temporal Gradient Matching Loss TGM that does not rely on optical flow, yet still ensures the temporal consistency of predictions between frames. OPW loss. To constrain temporal consistency, previous video models such as 19, 38, 39 assume that the depths at corresponding positions in adjacent frames, identified through optical flow, are consistent, e.g., the Optical Flow based Warping OPW loss proposed in NVDS 39 . OPW loss is computed after obtaining corresponding points on the basis of optical flow and warpi'}\n",
            "\n",
            "==================================================\n",
            "FINAL ANSWER\n",
            "==================================================\n",
            "The paper contains the following figures:\n",
            "1.  Figure 1 (extracted from 'arXiv Info and Figure 1', 'Introduction Experimental Results and Contributions')\n",
            "2.  Figure 2 (extracted from 'Video Depth Anything Overview', 'Architecture Spatiotemporal Head')\n",
            "\n",
            "The paper does not provide enough information regarding the presence of any tables.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/Video Depth Anything.pdf\"\n",
        "user_cmd = \"Tell a random bedtime story in short\"\n",
        "\n",
        "state: ResearchState = {\n",
        "    \"file_path\": pdf_path,\n",
        "    \"user_prompt\": user_cmd,\n",
        "    \"pdf_pages\": [],\n",
        "    \"split_sections\": {},\n",
        "    \"figures\": \"\",\n",
        "    \"plan\": \"\",\n",
        "    \"final_answer\": \"\"\n",
        "}\n",
        "\n",
        "try:\n",
        "    print(\"\\n\\n===== PIPELINE START =====\")\n",
        "    result = graph.invoke(state)\n",
        "    print(\"===== PIPELINE END =====\\n\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SECTION SPLITTER OUTPUT\")\n",
        "    print(\"=\"*50)\n",
        "    print(result.get(\"split_sections\", {}))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"FINAL ANSWER\")\n",
        "    print(\"=\"*50)\n",
        "    print(result.get(\"final_answer\", \"\"))\n",
        "\n",
        "except Exception:\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwZ2EJROuCTe",
        "outputId": "268e706f-acfb-400d-c551-6fd8a707e8ed"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "===== PIPELINE START =====\n",
            "\n",
            "[AGENT] pdf_loader running...\n",
            "[pdf_loader] Loaded PDF successfully.\n",
            "\n",
            "[SCHEDULER] Deciding next agent...\n",
            "\n",
            "[AGENT] splitter running...\n",
            "[LLM] Splitter agent calling LLM...\n",
            "[splitter] CLEANED keys: ['arXiv Info', 'Figure 1 Description', 'Abstract', 'Introduction Problem and Motivation', 'Introduction Our Approach Details', 'Introduction Results and Contributions', 'Related Work Monocular Depth Estimation', 'Related Work Consistent Video Depth Estimation', 'Video Depth Anything Overview', 'Architecture Depth Anything V2 Encoder', 'Architecture Spatiotemporal Head', 'Figure 2 Overall Pipeline', 'Temporal Gradient Matching Loss Introduction', 'Temporal Gradient Matching Loss OPW Loss']\n",
            "\n",
            "[AGENT] planner running...\n",
            "[LLM] Planner agent calling LLM...\n",
            "[planner] LLM call completed.\n",
            "\n",
            "[AGENT] final_answer running...\n",
            "[LLM] Final answer agent calling LLM...\n",
            "[final_answer] LLM call completed.\n",
            "===== PIPELINE END =====\n",
            "\n",
            "\n",
            "==================================================\n",
            "SECTION SPLITTER OUTPUT\n",
            "==================================================\n",
            "{'arXiv Info': 'arXiv 2501.12375v3 cs.CV 15 Jun 2025 Video Depth Anything Consistent Depth Estimation for Super-Long Videos Sili Chen Hengkai Guo Shengnan Zhu Feihu Zhang Zilong Huang Jiashi Feng Bingyi Kang ByteDance videodepthanything.github.io', 'Figure 1 Description': '3s 22s 42s 59s 79s 99s 122s 140s 161s 175s Figure 1. Left Our model can generate consistent depth predictions for long videos with rich actions. The demo video shows a 196-second 4690 frames long take of pair skating, as sourced from 14 . Right Comparison to baselines in terms of accuracy 1 , consistency, and latency on the Nvidia A100 GPU denoted with circle size . Consistency is defined as the maximum Temporal Alignment Error TAE among all models minus the TAE of each individual model. Our model achieves the best performance in all aspects.', 'Abstract': 'Abstract Depth Anything has achieved remarkable success in monocular depth estimation with strong generalization abil- ity. However, it suffers from temporal inconsistency in videos, hindering its practical applications. Various methods have been proposed to alleviate this issue by leveraging video gen- eration models or introducing priors from optical flow and camera poses. Nonetheless, these methods are only applica- ble to short videos 10 seconds and require a trade-off between quality and computational efficiency. We propose Video Depth Anything for high-quality, consistent depth esti- mation in super-long videos over several minutes without sacrificing efficiency. We base our model on Depth Anything V2 and replace its head with an efficient spatial-temporal head. We design a straightforward yet effective temporal consistency loss by constraining the temporal depth gradient, Corresponding author. eliminating the need for additional geometric priors. The model is trained on a joint dataset of video depth and unla- beled images, similar to Depth Anything V2. Moreover, a novel key-frame-based strategy is developed for long video inference. Experiments show that our model can be applied to arbitrarily long videos without compromising quality, con- sistency, or generalization ability. Comprehensive evalua- tions on multiple video benchmarks demonstrate that our approach sets a new state-of-the-art in zero-shot video depth estimation. We offer models of different scales to support a range of scenarios, with our smallest model capable of real-time performance at 30 FPS.', 'Introduction Problem and Motivation': '1. Introduction Recently, monocular depth estimation MDE has made sig- nificant progress, as evidenced by advances in depth foun- dation models 3, 17, 42, 43 . For example, Depth Any- thing V2 43 demonstrates a strong generalization ability in producing depth predictions with rich details in various scenarios while being computationally efficient. However, these models have a major limitation they are mainly de- signed for static images and suffer from flickering and mo- tion blur in videos. This limitation restricts their applications in robotics 8 , augmented reality 12 , and advanced video editing 25, 47 , which requires temporally consistent depth. Extensive efforts are being made to address this problem. Early work 18, 21, 48 often relies on test-time optimization to tune a pretrained monocular depth model with sophisti- cated geometry constraints. Given the heavy overhead at inference time of these methods, recent work mainly fo- cuses on feedforward models and can be categorized into two approaches. The first approach 39 involves design- ing a plug-and-play module to augment monocular depth model predictions with temporal consistency. The training of such a module is highly dependent on optical flow 40 or camera poses 30 for consistency constraints, making the module susceptible to corresponding errors. The second approach 13, 32, 41 repurposes pre-trained video diffusion models 4 into video-to-depth models. These methods excel at producing fine-grained details, but are computationally in- efficient, cannot leverage existing depth foundation models, and can only handle videos with limited length. Then, a natural question arises Is it possible to have a model that can perfectly inherit the capabilities of existing foundation models while achieving temporal stability for arbitrarily long videos In this paper, we show that the answer is YES by developing Video Depth Anythingbased on Depth Anything V2, without sacrificing its generalization ability, richness in details, or computational efficiency. This target is achieved without introducing any geometric priors or video generation priors.', 'Introduction Our Approach Details': 'Specifically, we first design a lightweight spatial-temporal head STH to replace the DPT head 28 and enable tem- poral information interactions. STH includes four temporal attention layers, applied along the temporal dimension for each spatial position. Introducing temporal attention only in the head prevents the learned representation from being corrupted by the limited video data. Then, we propose a temporal gradient matching loss to constrain depth predic- tion gradients along the temporal dimension to match those calculated from the ground truth. This loss function is jointly optimized with the scale-shift-invariant loss and spatial gra- dient matching loss 3, 42 . Despite its simplicity, it can effectively boost the model s temporal consistency. Third, to maintain the original capabilities of the model, we train it jointly on 730K video frames with depth annotations using supervised learning, and on 0.62 million unlabeled images using self-training, similar to Depth Anything V2 43 . To handle super-long videos at inference time, we developed a novel segment-wise processing strategy. Each new segment is concatenated with eight overlapping frames and two key frames from the previous video clips, forming a total of 32 frames. Then, the overlapping frames will be progressively interpolated between the two consecutive windows to ensure smoothness.', 'Introduction Results and Contributions': 'We compare our model with baselines on five datasets for zero-shot video depth estimation. Our model achieves state-of-the-art SOTA results on four of the datasets in terms of spatial accuracy and outperforms all baselines on all datasets in terms of temporal consistency. Not only can our model produce depth outputs visually comparable to video-diffusion-based methods, but it is also significantly more computationally efficient. For the first time, we can es- timate consistent depth for videos over several minutes see Fig. 1 . Additionally, we tested our model for zero-shot im- age depth estimation on five datasets, noting only a marginal performance drop on one dataset compared to Depth Any- thing V2. As shown in Fig. 1 right , our model achieves the best performance in all three aspects spatial accuracy, temporal consistency, and computational efficiency. Our contributions are summarized as follows We develop a novel method to transform Depth Anything into Video Depth Anythingfor depth estimation in arbitrar- ily long videos. We propose a simple yet effective loss function that en- forces temporal consistency constraints without introduc- ing geometric or generative priors. Our model not only sets new SOTA both spatially and temporally in video depth estimation but is also the most computationally efficient.', 'Related Work Monocular Depth Estimation': '2. Related Work Monocular depth estimation.Early monocular depth es- timation 1, 9, 10, 46 efforts were primarily trained and tested on in-domain datasets. These models were constrained by the limited diversity of their training data, showing bad generalization for zero-shot application. Subsequently, Mi- DaS 3 introduced multi-dataset mixed training using an affine-invariant alignment method, significantly enhancing model generalization. However, due to limitations in the backbone model s performance and noise in the labeled depth data, the resulting depth maps lacked fine details. Following MiDaS 3 , monocular depth estimation models have generally bifurcated into two categories relative depth models that estimate affine-invariant depth e.g., DPT 28 , DepthAnything 42, 43 , Marigold 17 and metric depth models that estimate depth with an absolute scale e.g., ZoeDepth 2 , Metric3D 45 , UniDepth 27 . Metric depth models require training with metric depth data that includes camera parameters 45 , thus their available training data is more limited compared to affine-invariant depth models, resulting in poorer generalization. Recently, Depth Anything V2 43 leveraged the Dinov2 23 pre-trained backbone to train an affine-invariant large-scale model using synthetic data with high-detail fidelity. This large model was then used as a teacher to distill smaller models on 62 million un- labeled datasets 42 , achieving SOTA performance in both generalization and geometric details. However, since Depth Anything V2 43 was trained exclusively on static images, thus lacks temporal consistency.', 'Related Work Consistent Video Depth Estimation': 'Consistent video depth estimation.The core task of consistent video depth estimation is to obtain temporal con- sistent and accuracy depth maps. Early methods for video depth relied on test-time training 18, 21, 48 , which were impractical for applications for their low efficiency. In re- cent years, learning-based models have emerged. Some of these models, such as MAMo 44 , use optical flow 33 to warp features, while others 29 depends on relative poses between frames to construct cost volumes. However, their performance were suffered from errors of inaccurate opti- cal flow or pose estimation. Additional approaches have attempted to enhance off-the-shelf monocular depth estima- tion MDE models with temporal stability model blocks 39 . Nevertheless, these efforts have not achieved satisfac- tory results due to suboptimal model designs and inadequate geometric consistency constraints. Furthermore, video diffu- sion models such as ChronoDepth 32 , DepthCrafter 13 , and DepthAnyVideo 41 show better details and temporal consistency. But they suffered from slow inference speeds and require extensive video depth training data. Limited by the large memory, these models 13 were typically tested only within the maximum window length used during train- ing, leading to depth flickering between windows and poor temporal and spatial consistency in long videos.', 'Video Depth Anything Overview': '3. Video Depth Anything In this section, we introduce Video Depth Anything, a feed- forward video transformer model to efficiently estimate tem- porally consistent video depth. We adopt the affine-invariant depth, but share the same scale and shift across the entire video. The pipeline of our method is shown in Fig. 2. Our model is built upon Depth Anything V2 with an additional temporal module and video dataset training Sec. 3.1 . A novel loss to enfoce temporal consistency is proposed in Sec. 3.2. Finally, a strategy combined with overlapping frames and key frames is presented to efficiently support super-long video inference Sect. 3.3 .', 'Architecture Depth Anything V2 Encoder': '3.1. Architecture Due to the lack of sufficient video depth data, we start with a pre-trained image depth estimation model, Depth Anything V2, and adopt a joint training strategy using both image and video data. Depth Anything V2 Encoder.Depth Anything V2 43 is the current state-of-the-art monocular depth estimation model, characterized by its high accuracy and generalization capabilities. We use its trained model as our encoder. To reduce training costs and preserve well-learned features, the encoder is frozen during training. Unlike monocular depth encoders that only accept image input, our training scenario requires the encoder to process simultaneously both video and image data. To extract fea- tures from video frames with an image encoder, we collapse the temporal dimension of a video clip into the batch dimen- sion. The input data are denoted as X R B N C H W , where B represents the batch size,N is the number of frames in the video clip, N 1 for the image as input, C, H, W are the number of channels, height, width of the frames, re- spectively. The encoder takes X as input to produce a series of intermediate feature maps Fi R B N H p W p Ci, p is the patch size of the encoder. Although the image en- coder extracts strong visual representations from individual frames, it neglects the temporal information interactions be- tween frames. Thus, the spatiotemporal head is introduced to model the temporal relationship among the frames.', 'Architecture Spatiotemporal Head': 'Spatiotemporal Head.The spatiotemporal head STH is built upon the DPT 28 head and with the only modifica- tion being the insertion of temporal layers to capture tempo- ral information. A temporal layer consists of a multi-head self-attention 34 model SA and a feed-forward network FFN . When inputting a feature Fi into the temporal layer, the temporal dimension N is isolated, and self-attention is executed solely along the temporal dimension to facilitate the interaction of temporal features. To capture temporal positional relationships among different frames, we utilize absolute positional embedding to encode temporal positional information from the video sequence. The spatiotemporal head uniformly samples 4 feature maps from Fi including the final features from the encoder, denoted as F4 as inputs, and predicts a depth map D RH W . As shown in Figure 2, the selected features Fi are fed into the Reassemble layer to produce a feature pyramid. Then, the features are gradually fused from low resolution to high resolution by the Fusion layer. The Reassemble and Fusion layer are proposed by DPT 28 . The final fused high-resolution feature maps are passed through the output layer to produce the depth map D. To reduce the additional computational load, we insert the temporal layer at a few positions with lower feature resolutions.', 'Figure 2 Overall Pipeline': 'Figure 2. Overall pipeline and the spatio-temporal head. Left Our model is composed of a backbone encoder from Depth Anything V2 and a newly proposed spatio-temporal head. We jointly train our model on video data using ground-truth depth labels for supervision and on unlabeled images with pseudo labels generated by a teacher model. During training, only the head is learned. Right Our spatiotemporal head inserts several temporal layers into the DPT head, while preserving the original structure of DPT head 28 .', 'Temporal Gradient Matching Loss Introduction': '3.2. Temporal Gradient Matching loss In this section, we start with the Optical Flow Based Warping OPW loss, then explore new loss designs and ultimately propose a Temporal Gradient Matching Loss TGM that does not rely on optical flow, yet still ensures the temporal consistency of predictions between frames.', 'Temporal Gradient Matching Loss OPW Loss': 'OPW loss. To constrain temporal consistency, previous video models such as 19, 38, 39 assume that the depths at corresponding positions in adjacent frames, identified through optical flow, are consistent, e.g., the Optical Flow based Warping OPW loss proposed in NVDS 39 . OPW loss is computed after obtaining corresponding points on the basis of optical flow and warpi'}\n",
            "\n",
            "==================================================\n",
            "FINAL ANSWER\n",
            "==================================================\n",
            "The paper does not provide enough information to fulfill this request.\n"
          ]
        }
      ]
    }
  ]
}